{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Lib imports\n",
    "import re\n",
    "import os\n",
    "import html\n",
    "import pickle\n",
    "import sklearn\n",
    "import collections\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "os.environ['QT_QPA_PLATFORM']='offscreen'\n",
    "\n",
    "# FastAI Imports\n",
    "from fastai import text, core, lm_rnn\n",
    "\n",
    "# Torch imports\n",
    "import torch.nn as nn\n",
    "import torch.tensor as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Mytorch imports\n",
    "from mytorch import loops, lriters as mtlr\n",
    "from mytorch.utils.goodies import *\n",
    "\n",
    "# Local imports\n",
    "from options import Phase2 as params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "class CustomEncoder(lm_rnn.RNN_Encoder):\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return torch.nn.ModuleList([torch.nn.ModuleList([self.rnns[0], self.dropouths[0]]),\n",
    "                                    torch.nn.ModuleList([self.rnns[1], self.dropouths[1]]),\n",
    "                                    torch.nn.ModuleList([self.rnns[2], self.dropouths[2]])])\n",
    "\n",
    "\n",
    "class CustomLinear(text.LinearDecoder):\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return torch.nn.ModuleList([self.decoder, self.dropout])\n",
    "\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 _parameter_dict,\n",
    "                 _device,\n",
    "                 _wgts_e,\n",
    "                 _wgts_d,\n",
    "                 _encargs):\n",
    "        super(LanguageModel, self).__init__()\n",
    "\n",
    "        self.parameter_dict = _parameter_dict\n",
    "        self.device = _device\n",
    "\n",
    "        self.encoder = CustomEncoder(**_encargs).to(self.device)\n",
    "        self.encoder.load_state_dict(_wgts_e)\n",
    "        \"\"\"\n",
    "            Explanation:\n",
    "                400*3 because input is [ h_T, maxpool, meanpool ]\n",
    "                0.4, 0.1 are drops at various layersLM_PATH\n",
    "        \"\"\"\n",
    "        self.linear = CustomLinear(\n",
    "            _encargs['ntoken'],\n",
    "            n_hid=400,\n",
    "            dropout=0.1 * 0.7,\n",
    "            tie_encoder=self.encoder.encoder,\n",
    "            bias=False\n",
    "        ).to(self.device)\n",
    "        self.encoder.reset()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding all the data\n",
    "        op_p = self.encoder(x)\n",
    "\n",
    "        # pos_batch = op_p[1][-1][-1]\n",
    "        score = self.linear(op_p)[0]\n",
    "\n",
    "        return score\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        layers = [x for x in self.encoder.layers]\n",
    "        layers += [x for x in self.linear.layers]\n",
    "        return torch.nn.ModuleList(layers)\n",
    "\n",
    "    @property\n",
    "    def layers_rev(self):\n",
    "        layers = [x for x in self.encoder.layers]\n",
    "        layers += [x for x in self.linear.layers]\n",
    "        layers.reverse()\n",
    "        return torch.nn.ModuleList(layers)\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            pred = self.forward(x)\n",
    "            self.train()\n",
    "            return pred\n",
    "\n",
    "\n",
    "def eval(y_pred, y_true):\n",
    "    \"\"\"\n",
    "        Expects a batch of input\n",
    "\n",
    "        :param y_pred: tensor of shape (b, nc)\n",
    "        :param y_true: tensor of shape (b, 1)\n",
    "    \"\"\"\n",
    "    return torch.mean((torch.argmax(y_pred, dim=1) == y_true).float())\n",
    "\n",
    "\n",
    "# Path fields\n",
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "\n",
    "DATA_PATH = Path('raw/imdb/aclImdb/')\n",
    "DATA_PATH.mkdir(exist_ok=True)\n",
    "PATH = Path('resources/proc/imdb')\n",
    "DATA_PROC_PATH = PATH / 'data'\n",
    "DATA_LM_PATH = PATH / 'datalm'\n",
    "\n",
    "LM_PATH = Path('resources/models')\n",
    "LM_PATH.mkdir(exist_ok=True)\n",
    "PRE_PATH = LM_PATH / 'wt103'\n",
    "PRE_LM_PATH = PRE_PATH / 'fwd_wt103.h5'\n",
    "CLASSES = ['neg', 'pos', 'unsup']\n",
    "\n",
    "\n",
    "def get_texts_org(path):\n",
    "    texts, labels = [], []\n",
    "    for idx, label in enumerate(CLASSES):\n",
    "        for fname in (path / label).glob('*.*'):\n",
    "            texts.append(fname.open('r', encoding='utf-8').read())\n",
    "            labels.append(idx)\n",
    "    return np.array(texts), np.array(labels)\n",
    "\n",
    "\n",
    "def _fixup_(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
    "        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "\n",
    "def _get_texts_(df, n_lbls=1):\n",
    "    labels = df.iloc[:, range(n_lbls)].values.astype(np.int64)\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)\n",
    "    for i in range(n_lbls + 1, len(df.columns)): texts += f' {FLD} {i-n_lbls} ' + df[i].astype(str)\n",
    "    texts = list(texts.apply(_fixup_).values)\n",
    "\n",
    "    tok = text.Tokenizer().proc_all_mp(core.partition_by_cores(texts))\n",
    "    return tok, list(labels)\n",
    "\n",
    "\n",
    "def get_all(df, n_lbls):\n",
    "    tok, labels = [], []\n",
    "    for i, r in enumerate(df):\n",
    "        print(i)\n",
    "        tok_, labels_ = _get_texts_(r, n_lbls)\n",
    "        tok += tok_;\n",
    "        labels += labels_\n",
    "    return tok, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIM = True\n",
    "DEBUG = True\n",
    "\n",
    "trn_texts, trn_labels = get_texts_org(DATA_PATH / 'train')\n",
    "val_texts, val_labels = get_texts_org(DATA_PATH / 'test')\n",
    "\n",
    "if TRIM:\n",
    "    trn_texts, val_texts = trn_texts[:1000], val_texts[:1000]\n",
    "\n",
    "col_names = ['labels', 'text']\n",
    "\n",
    "if DEBUG:\n",
    "    print(len(trn_texts), len(val_texts))\n",
    "\n",
    "# Shuffle data\n",
    "np.random.seed(42)\n",
    "trn_idx = np.random.permutation(len(trn_texts))\n",
    "val_idx = np.random.permutation(len(val_texts))\n",
    "\n",
    "trn_texts, trn_labels = trn_texts[trn_idx], trn_labels[trn_idx]\n",
    "val_texts, val_labels = val_texts[val_idx], val_labels[val_idx]\n",
    "\n",
    "df_trn = pd.DataFrame({'text': trn_texts, 'labels': trn_labels}, columns=col_names)\n",
    "df_val = pd.DataFrame({'text': val_texts, 'labels': val_labels}, columns=col_names)\n",
    "\n",
    "df_trn[df_trn['labels'] != 2].to_csv(DATA_PROC_PATH / 'train.csv', header=False, index=False)\n",
    "df_val.to_csv(DATA_PROC_PATH / 'test.csv', header=False, index=False)\n",
    "\n",
    "(DATA_PROC_PATH / 'classes.txt').open('w', encoding='utf-8').writelines(f'{o}\\n' for o in CLASSES)\n",
    "\n",
    "trn_texts, val_texts = sklearn.model_selection.train_test_split(\n",
    "    np.concatenate([trn_texts, val_texts]), test_size=0.1)\n",
    "\n",
    "if DEBUG:\n",
    "    print(len(trn_texts), len(val_texts))\n",
    "\n",
    "df_trn = pd.DataFrame({'text': trn_texts, 'labels': [0] * len(trn_texts)}, columns=col_names)\n",
    "df_val = pd.DataFrame({'text': val_texts, 'labels': [0] * len(val_texts)}, columns=col_names)\n",
    "\n",
    "df_trn.to_csv(DATA_LM_PATH / 'train.csv', header=False, index=False)\n",
    "df_val.to_csv(DATA_LM_PATH / 'test.csv', header=False, index=False)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    ## Language model tokens\n",
    "\n",
    "    In this section, we start cleaning up the messy text. There are 2 main activities we need to perform:\n",
    "\n",
    "    1. Clean up extra spaces, tab chars, new ln chars and other characters and replace them with standard ones\n",
    "    2. Use the awesome [spacy](http://spacy.io) library to tokenize the data. \n",
    "    Since spacy does not provide a parallel/multicore version of the tokenizer, \n",
    "        the fastai library adds this functionality. \n",
    "    This parallel version uses all the cores of your CPUs \n",
    "        and runs much faster than the serial version of the spacy tokenizer.\n",
    "\n",
    "    Tokenization is the process of splitting the text into separate tokens \n",
    "        so that each token can be assigned a unique index. \n",
    "    This means we can convert the text into integer indexes our models can use.\n",
    "\n",
    "    We use an appropriate chunksize as the tokenization process is memory intensive\n",
    "\"\"\"\n",
    "chunksize = 24000\n",
    "re1 = re.compile(r'  +')\n",
    "\n",
    "df_trn = pd.read_csv(DATA_LM_PATH / 'train.csv', header=None, chunksize=chunksize)\n",
    "df_val = pd.read_csv(DATA_LM_PATH / 'test.csv', header=None, chunksize=chunksize)\n",
    "\n",
    "tok_trn, trn_labels = get_all(df_trn, 1)\n",
    "tok_val, val_labels = get_all(df_val, 1)\n",
    "\n",
    "# Save to disk\n",
    "(DATA_LM_PATH / 'tmp').mkdir(exist_ok=True)\n",
    "np.save(DATA_LM_PATH / 'tmp' / 'tok_trn.npy', tok_trn)\n",
    "np.save(DATA_LM_PATH / 'tmp' / 'tok_val.npy', tok_val)\n",
    "tok_trn = np.load(DATA_LM_PATH / 'tmp' / 'tok_trn.npy')\n",
    "tok_val = np.load(DATA_LM_PATH / 'tmp' / 'tok_val.npy')\n",
    "\n",
    "freq = Counter(p for o in tok_trn for p in o)\n",
    "# freq.most_common(25)\n",
    "max_vocab = params.max_vocab_task\n",
    "min_freq = 2\n",
    "\n",
    "itos = [o for o, c in freq.most_common(max_vocab) if c > min_freq]\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(0, '_unk_')\n",
    "\n",
    "stoi = collections.defaultdict(lambda: 0, {v: k for k, v in enumerate(itos)})\n",
    "if DEBUG:\n",
    "    print(len(itos))\n",
    "\n",
    "trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_lm = np.array([[stoi[o] for o in p] for p in tok_val])\n",
    "np.save(DATA_LM_PATH / 'tmp' / 'trn_ids.npy', trn_lm)\n",
    "np.save(DATA_LM_PATH / 'tmp' / 'val_ids.npy', val_lm)\n",
    "pickle.dump(itos, open(DATA_LM_PATH / 'tmp' / 'itos.pkl', 'wb'))\n",
    "trn_lm = np.load(DATA_LM_PATH / 'tmp' / 'trn_ids.npy')\n",
    "val_lm = np.load(DATA_LM_PATH / 'tmp' / 'val_ids.npy')\n",
    "itos = pickle.load(open(DATA_LM_PATH / 'tmp' / 'itos.pkl', 'rb'))\n",
    "vs = len(itos)\n",
    "\n",
    "if DEBUG:\n",
    "    print(vs, len(trn_lm))\n",
    "\n",
    "\"\"\"\n",
    "    Now we pull pretrained models from disk\n",
    "\"\"\"\n",
    "em_sz, nh, nl = 400, 1150, 3\n",
    "# PRE_PATH = PATH / 'models' / 'wt103'\n",
    "# PRE_LM_PATH = PRE_PATH / 'fwd_wt103.h5'\n",
    "wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)\n",
    "enc_wgts = core.to_np(wgts['0.encoder.weight'])\n",
    "row_m = enc_wgts.mean(0)\n",
    "itos2 = pickle.load((PRE_PATH / 'itos_wt103.pkl').open('rb'))\n",
    "stoi2 = collections.defaultdict(lambda: -1, {v: k for k, v in enumerate(itos2)})\n",
    "new_w = np.zeros((vs, em_sz), dtype=np.float32)\n",
    "for i, w in enumerate(itos):\n",
    "    r = stoi2[w]\n",
    "    new_w[i] = enc_wgts[r] if r >= 0 else row_m\n",
    "\n",
    "wgts['0.encoder.weight'] = T(new_w)\n",
    "wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))\n",
    "wgts['1.decoder.weight'] = T(np.copy(new_w))\n",
    "wgts_enc = {'.'.join(k.split('.')[1:]): val\n",
    "            for k, val in wgts.items() if k[0] == '0'}\n",
    "wgts_dec = {'.'.join(k.split('.')[1:]): val\n",
    "            for k, val in wgts.items() if k[0] == '1'}\n",
    "\n",
    "wd = 1e-7\n",
    "bptt = 70\n",
    "bs = params.bs\n",
    "opt_fn = partial(torch.optim.Adam, betas=params.adam_betas)  # @TODO: find real optimizer, and params\n",
    "\n",
    "# Load the pre-trained model\n",
    "parameter_dict = {'itos2': itos2}\n",
    "dps = params.encoder_drops\n",
    "encargs = {'ntoken': new_w.shape[0],\n",
    "           'emb_sz': 400, 'n_hid': 1150,\n",
    "           'n_layers': 3, 'pad_token': 0,\n",
    "           'qrnn': False, 'dropouti': dps[0],\n",
    "           'wdrop': dps[2], 'dropoute': dps[3], 'dropouth': dps[4]}\n",
    "\n",
    "lm = LanguageModel(parameter_dict, device, wgts_enc, wgts_dec, encargs)\n",
    "opt = make_opt(lm, opt_fn, lr=params.lr.init)\n",
    "\n",
    "data_fn = partial(text.LanguageModelLoader, bs=bs, bptt=bptt)\n",
    "data = {'train': np.concatenate(trn_lm), 'valid': np.concatenate(val_lm)}\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "'''\n",
    "    Schedule\n",
    "\n",
    "    -> Freeze all but last layer, run for 1 epoch\n",
    "    -> Unfreeze all of it, and apply discriminative fine-tuning, train normally.\n",
    "'''\n",
    "for grp in opt.param_groups:\n",
    "    grp['lr'] = 0.0\n",
    "opt.param_groups[0]['lr'] = params.lr.init\n",
    "\n",
    "# lr_args = {'batches':, 'cycles': 1}\n",
    "lr_args = {'iterations': len(data_fn(data['train']))*1, 'cut_frac': params.lr.sltr_cutfrac, 'ratio': params.lr.sltr_ratio}\n",
    "lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.SlantedTriangularLR)\n",
    "\n",
    "# Find places to save model\n",
    "save_dir = mt_save_dir(PATH / 'models', _newdir=True)\n",
    "\n",
    "# Start to put permanent things there, like the itos\n",
    "mt_save(save_dir,\n",
    "        pickle_stuff=[tosave('itos.pkl', itos)])\n",
    "\n",
    "args = {'epochs': 1, 'weight_decay': params.weight_decay, 'data': data,\n",
    "        'device': device, 'opt': opt, 'loss_fn': loss_fn, 'train_fn': lm,\n",
    "        'predict_fn': lm.predict, 'data_fn': data_fn, 'model': lm,\n",
    "        'eval_fn': eval, 'epoch_start_hook': partial(loops.reset_hidden, lm),\n",
    "        'clip_grads_at': params.clip_grads_at, 'lr_schedule': lr_schedule}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_start = loops.generic_loop(**args)\n",
    "\n",
    "# Now unfreeze all layers and apply discr\n",
    "for grp in opt.param_groups:\n",
    "    grp['lr'] = params.lr.init\n",
    "\n",
    "lr_dscr = lambda opt, lr, fctr=2.6: [lr / (fctr ** i) for i in range(len(opt.param_groups))[::-1]]\n",
    "update_lr(opt, lr_dscr(opt, params.lr.init))\n",
    "\n",
    "if DEBUG:\n",
    "    print([x['lr'] for x in opt.param_groups])\n",
    "\n",
    "lr_args = {'iterations': len(data_fn(data['train']))*15, 'cut_frac': params.lr.sltr_cutfrac, 'ratio': params.lr.sltr_ratio}\n",
    "lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.SlantedTriangularLR)\n",
    "args['lr_schedule'] = lr_schedule\n",
    "args['epochs'] = 15\n",
    "\n",
    "traces_main = loops.generic_loop(**args)\n",
    "traces = [a+b for a, b in zip(traces_start, traces_main)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot  Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_accs(tra, vla, axa):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "#     ax = fig.add_axes((0.1, 0.2, 0.8, 0.7))\n",
    "#     ax.spines['right'].set_color('none')\n",
    "#     ax.spines['top'].set_color('none')\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "    plt.plot(tra, label=f\"Train Acc\", linewidth=3)\n",
    "    plt.plot(vla, label=f\"Valid Acc\", linewidth=3)\n",
    "    plt.plot(axa, label=f\"DAdNN Acc\", linewidth=3)\n",
    "    plt.show()\n",
    "    \n",
    "plot_accs(traces[0], traces[1], traces[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot LRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(trcs):\n",
    "    layers = len(trcs[0])\n",
    "    for l in range(layers):\n",
    "        plt.plot(trcs[:,l], label=f\"layer {l}\")\n",
    "    plt.show()\n",
    "    \n",
    "plot(np.asarray(traces[-1][100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final save, just in case\n",
    "# Dumping stuff\n",
    "mt_save(save_dir,\n",
    "        torch_stuff=[tosave('unsup_model_enc.torch', lm.encoder.state_dict()), tosave('unsup_model.torch', lm.state_dict())],\n",
    "        pickle_stuff=[tosave('final_unsup_traces.pkl', traces), tosave('unsup_options.pkl', params)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
