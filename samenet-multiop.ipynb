{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# pretty print all cell's output and not just the last one\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Lib imports\n",
    "import collections\n",
    "import html\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ['QT_QPA_PLATFORM'] = 'offscreen'\n",
    "\n",
    "# FastAI Imports\n",
    "from fastai import text, core, lm_rnn\n",
    "\n",
    "# Torch imports\n",
    "import torch.nn as nn\n",
    "import torch.tensor as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Mytorch imports\n",
    "from mytorch import loops as mtlp\n",
    "from mytorch.utils.goodies import *\n",
    "from mytorch import lriters as mtlr\n",
    "\n",
    "import utils\n",
    "from options import Options as params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f862d696590>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "DEBUG = True\n",
    "TRIM=False\n",
    "\n",
    "# Path fields\n",
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "\n",
    "WIKI_DATA_PATH = Path('raw/wikitext/wikitext-103/')\n",
    "WIKI_DATA_PATH.mkdir(exist_ok=True)\n",
    "IMDB_DATA_PATH = Path('raw/imdb/aclImdb/')\n",
    "IMDB_DATA_PATH.mkdir(exist_ok=True)\n",
    "PATH = Path('resources/proc/imdb')\n",
    "DATA_PROC_PATH = PATH / 'data'\n",
    "DATA_LM_PATH = PATH / 'datalm'\n",
    "\n",
    "LM_PATH = Path('resources/models')\n",
    "LM_PATH.mkdir(exist_ok=True)\n",
    "PRE_PATH = LM_PATH / 'wt103'\n",
    "PRE_LM_PATH = PRE_PATH / 'fwd_wt103.h5'\n",
    "CLASSES = ['neg', 'pos', 'unsup']\n",
    "WIKI_CLASSES = ['wiki.train.tokens', 'wiki.valid.tokens', 'wiki.test.tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network\n",
    "\n",
    "Three part network,\n",
    " - a feature extractor\n",
    " - a label predictor\n",
    " - a domain classifier\n",
    " \n",
    " Ref:\n",
    " bs = 10\n",
    " sl = 5\n",
    " hiddim = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 10\n",
    "sl = 5\n",
    "hdim = 20\n",
    "\n",
    "np_x = np.random.randint(0, 1000, (sl, bs))\n",
    "np_y = np.random.randint(0, 3, (bs))\n",
    "np_d = np.random.randint(0, 2, (bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     22,
     35,
     40
    ]
   },
   "outputs": [],
   "source": [
    "class FeatExtractor(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(1000, 20)\n",
    "        self.rnn = nn.LSTM(20, 20)\n",
    "        \n",
    "    def init_hidden(self, bs):\n",
    "        return (torch.zeros((1, bs, 20)),\n",
    "                torch.zeros((1, bs, 20)))\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        x_emb = self.emb(x)\n",
    "        print(x_emb.shape, h[0].shape, h[1].shape)\n",
    "        x, h = self.rnn(x_emb, h)\n",
    "        return x, h\n",
    "    \n",
    "    @property\n",
    "    def layers(self):\n",
    "        return torch.nn.ModuleList([\n",
    "            self.emb, self.rnn\n",
    "        ])    \n",
    "class LabelPredictor(nn.Module):\n",
    "    \n",
    "    def __init__(self, out):\n",
    "        super().__init__()\n",
    "        self.clf = nn.Linear(sl*hdim, out)\n",
    "        \n",
    "    @property\n",
    "    def layers(self):\n",
    "        return torch.nn.ModuleList([self.clf])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return x, self.clf(x)   \n",
    "class DomainClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.clf = nn.Linear(sl*hdim, out)    \n",
    "class GradReverse(Function):\n",
    "    \"\"\"\n",
    "        Torch function used to invert the sign of gradients (to be used for argmax instead of argmin)\n",
    "        Usage:\n",
    "            x = GradReverse.apply(x) where x is a tensor with grads.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg()  \n",
    "class ZeNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.f = FeatExtractor()\n",
    "        self.y = LabelPredictor(3)\n",
    "        self.d = LabelPredictor(2)\n",
    "        \n",
    "    @property\n",
    "    def layers(self):\n",
    "        return self.f.layers.extend(self.y.layers.extend(self.d.layers))\n",
    "    \n",
    "    def domain_(self, x):\n",
    "        h = self.f.init_hidden(x.shape[1])\n",
    "        x, h = self.f(x, h)\n",
    "        \n",
    "        x = x.transpose(1, 0).reshape(h[0].shape[1], -1)\n",
    "        \n",
    "        x, d = self.d(x)\n",
    "        return x, d\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.f.init_hidden(x.shape[1])\n",
    "        x, h = self.f(x, h)\n",
    "        \n",
    "        x = x.transpose(1, 0).reshape(h[0].shape[1], -1)\n",
    "        \n",
    "        x, y = self.y(x)\n",
    "#         print(y.shape)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def domain(self, x):\n",
    "        x = GradReverse.apply(x)\n",
    "        x, y = self.d(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Embedding(1000, 20)\n",
       "  (1): LSTM(20, 20)\n",
       "  (2): Linear(in_features=100, out_features=3, bias=True)\n",
       "  (3): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flow = 'dann'\n",
    "# assert flow  in ['main', 'dann', 'both']\n",
    "\n",
    "model = ZeNetwork()\n",
    "model.layers\n",
    "lfn = nn.CrossEntropyLoss()\n",
    "_x = torch.tensor(np_x)\n",
    "_y = torch.tensor(np_y)\n",
    "_d = torch.tensor(np_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 2, 0, 2, 0, 2, 2, 0, 0]),\n",
       " tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_y, _d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 20]) torch.Size([1, 10, 20]) torch.Size([1, 10, 20])\n",
      "tensor(0.4872, grad_fn=<MulBackward>)\n"
     ]
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "_x = torch.tensor(np_x)\n",
    "_y = torch.tensor(np_y)\n",
    "_d = torch.tensor(np_d)\n",
    "\n",
    "x, y = model(_x)\n",
    "l_main = lfn(y, _y)\n",
    "l_main.backward(retain_graph=True)\n",
    "\n",
    "x, d = model.domain(x)\n",
    "l_dann = lfn(d, _d) * 0.7\n",
    "# l = l_main + ( 0.7 * l_dann)\n",
    "l_dann.backward()\n",
    "\n",
    "grads_dann_f = [param.grad.clone() for param in model.f.rnn.parameters()]\n",
    "grads_dann_d = [param.grad.clone() for param in model.d.clf.parameters()]\n",
    "grads_dann_y = [param.grad.clone() for param in model.y.clf.parameters()]\n",
    "print(l_dann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.0124,  0.0037, -0.0105,  0.0018, -0.0108,  0.0021,  0.0137,  0.0075,\n",
       "          -0.0037, -0.0106, -0.0040, -0.0119, -0.0124, -0.0106,  0.0175, -0.0163,\n",
       "          -0.0072,  0.0035, -0.0238, -0.0207, -0.0132,  0.0086, -0.0199,  0.0155,\n",
       "          -0.0056,  0.0285,  0.0164, -0.0263, -0.0060, -0.0204,  0.0074, -0.0209,\n",
       "           0.0099, -0.0207,  0.0065, -0.0158,  0.0020, -0.0121, -0.0140, -0.0101,\n",
       "          -0.0336, -0.0009, -0.0241,  0.0017, -0.0134,  0.0136, -0.0021, -0.0229,\n",
       "          -0.0135,  0.0086, -0.0050,  0.0034,  0.0112, -0.0127, -0.0214, -0.0013,\n",
       "          -0.0087,  0.0030, -0.0115, -0.0089, -0.0071, -0.0012, -0.0014, -0.0091,\n",
       "          -0.0131,  0.0038,  0.0038, -0.0201, -0.0071, -0.0070, -0.0031,  0.0116,\n",
       "          -0.0127,  0.0079,  0.0093, -0.0050, -0.0200, -0.0103, -0.0258, -0.0062,\n",
       "           0.0157, -0.0019,  0.0000, -0.0061,  0.0030, -0.0156, -0.0120, -0.0351,\n",
       "          -0.0020, -0.0018, -0.0212, -0.0026,  0.0072,  0.0130, -0.0183,  0.0073,\n",
       "          -0.0243,  0.0171,  0.0257,  0.0128],\n",
       "         [ 0.0124, -0.0037,  0.0105, -0.0018,  0.0108, -0.0021, -0.0137, -0.0075,\n",
       "           0.0037,  0.0106,  0.0040,  0.0119,  0.0124,  0.0106, -0.0175,  0.0163,\n",
       "           0.0072, -0.0035,  0.0238,  0.0207,  0.0132, -0.0086,  0.0199, -0.0155,\n",
       "           0.0056, -0.0285, -0.0164,  0.0263,  0.0060,  0.0204, -0.0074,  0.0209,\n",
       "          -0.0099,  0.0207, -0.0065,  0.0158, -0.0020,  0.0121,  0.0140,  0.0101,\n",
       "           0.0336,  0.0009,  0.0241, -0.0017,  0.0134, -0.0136,  0.0021,  0.0229,\n",
       "           0.0135, -0.0086,  0.0050, -0.0034, -0.0112,  0.0127,  0.0214,  0.0013,\n",
       "           0.0087, -0.0030,  0.0115,  0.0089,  0.0071,  0.0012,  0.0014,  0.0091,\n",
       "           0.0131, -0.0038, -0.0038,  0.0201,  0.0071,  0.0070,  0.0031, -0.0116,\n",
       "           0.0127, -0.0079, -0.0093,  0.0050,  0.0200,  0.0103,  0.0258,  0.0062,\n",
       "          -0.0157,  0.0019, -0.0000,  0.0061, -0.0030,  0.0156,  0.0120,  0.0351,\n",
       "           0.0020,  0.0018,  0.0212,  0.0026, -0.0072, -0.0130,  0.0183, -0.0073,\n",
       "           0.0243, -0.0171, -0.0257, -0.0128]]), tensor([ 0.1395, -0.1395])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads_dann_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 20]) torch.Size([1, 10, 20]) torch.Size([1, 10, 20])\n",
      "tensor(0.6961, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5924, grad_fn=<ThAddBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "_x = torch.tensor(np_x)\n",
    "_y = torch.tensor(np_y)\n",
    "_d = torch.tensor(np_d)\n",
    "\n",
    "x, y = model(_x)\n",
    "l_main = lfn(y, _y)\n",
    "# l_main.backward(retain_graph=True)\n",
    "\n",
    "x, d = model.domain(x)\n",
    "l_dann = lfn(d, _d)\n",
    "l = l_main + (l_dann * 0.7)\n",
    "l.backward()\n",
    "\n",
    "grads_dann_f = [param.grad.clone() for param in model.f.rnn.parameters()]\n",
    "grads_dann_d = [param.grad.clone() for param in model.d.clf.parameters()]\n",
    "grads_dann_y = [param.grad.clone() for param in model.y.clf.parameters()]\n",
    "print(l_dann), print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.0124,  0.0037, -0.0105,  0.0018, -0.0108,  0.0021,  0.0137,  0.0075,\n",
       "          -0.0037, -0.0106, -0.0040, -0.0119, -0.0124, -0.0106,  0.0175, -0.0163,\n",
       "          -0.0072,  0.0035, -0.0238, -0.0207, -0.0132,  0.0086, -0.0199,  0.0155,\n",
       "          -0.0056,  0.0285,  0.0164, -0.0263, -0.0060, -0.0204,  0.0074, -0.0209,\n",
       "           0.0099, -0.0207,  0.0065, -0.0158,  0.0020, -0.0121, -0.0140, -0.0101,\n",
       "          -0.0336, -0.0009, -0.0241,  0.0017, -0.0134,  0.0136, -0.0021, -0.0229,\n",
       "          -0.0135,  0.0086, -0.0050,  0.0034,  0.0112, -0.0127, -0.0214, -0.0013,\n",
       "          -0.0087,  0.0030, -0.0115, -0.0089, -0.0071, -0.0012, -0.0014, -0.0091,\n",
       "          -0.0131,  0.0038,  0.0038, -0.0201, -0.0071, -0.0070, -0.0031,  0.0116,\n",
       "          -0.0127,  0.0079,  0.0093, -0.0050, -0.0200, -0.0103, -0.0258, -0.0062,\n",
       "           0.0157, -0.0019,  0.0000, -0.0061,  0.0030, -0.0156, -0.0120, -0.0351,\n",
       "          -0.0020, -0.0018, -0.0212, -0.0026,  0.0072,  0.0130, -0.0183,  0.0073,\n",
       "          -0.0243,  0.0171,  0.0257,  0.0128],\n",
       "         [ 0.0124, -0.0037,  0.0105, -0.0018,  0.0108, -0.0021, -0.0137, -0.0075,\n",
       "           0.0037,  0.0106,  0.0040,  0.0119,  0.0124,  0.0106, -0.0175,  0.0163,\n",
       "           0.0072, -0.0035,  0.0238,  0.0207,  0.0132, -0.0086,  0.0199, -0.0155,\n",
       "           0.0056, -0.0285, -0.0164,  0.0263,  0.0060,  0.0204, -0.0074,  0.0209,\n",
       "          -0.0099,  0.0207, -0.0065,  0.0158, -0.0020,  0.0121,  0.0140,  0.0101,\n",
       "           0.0336,  0.0009,  0.0241, -0.0017,  0.0134, -0.0136,  0.0021,  0.0229,\n",
       "           0.0135, -0.0086,  0.0050, -0.0034, -0.0112,  0.0127,  0.0214,  0.0013,\n",
       "           0.0087, -0.0030,  0.0115,  0.0089,  0.0071,  0.0012,  0.0014,  0.0091,\n",
       "           0.0131, -0.0038, -0.0038,  0.0201,  0.0071,  0.0070,  0.0031, -0.0116,\n",
       "           0.0127, -0.0079, -0.0093,  0.0050,  0.0200,  0.0103,  0.0258,  0.0062,\n",
       "          -0.0157,  0.0019, -0.0000,  0.0061, -0.0030,  0.0156,  0.0120,  0.0351,\n",
       "           0.0020,  0.0018,  0.0212,  0.0026, -0.0072, -0.0130,  0.0183, -0.0073,\n",
       "           0.0243, -0.0171, -0.0257, -0.0128]]), tensor([ 0.1395, -0.1395])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads_dann_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD SHIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "x, y = model(_x)\n",
    "l_main = lfn(y, _y)\n",
    "l_main.backward()\n",
    "grads_main_f = [param.grad.clone() for param in model.f.rnn.parameters()]\n",
    "print(l_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "x, d = model.domain_(_x)\n",
    "l_dann = lfn(d, _d)\n",
    "l_dann.backward()\n",
    "grads_dann_f = [param.grad.clone() for param in model.f.rnn.parameters()]\n",
    "print(l_dann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "_x = torch.tensor(np_x)\n",
    "_y = torch.tensor(np_y)\n",
    "_d = torch.tensor(np_d)\n",
    "\n",
    "x, y = model(_x)\n",
    "l_main = lfn(y, _y)\n",
    "# l_main.backward(retain_graph=True)\n",
    "\n",
    "x, d = model.domain(x)\n",
    "l_dann = lfn(d, _d)\n",
    "l = l_main + ( 0.7 * l_dann)\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_main_f[0] + (0.7*grads_dann_f[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[param.grad for param in model.f.rnn.parameters()][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion!\n",
    "\n",
    "What I find is, that the best way to do the DANN updates, i.e.\n",
    "\n",
    "$ \\theta_f \\leftarrow \\theta_f - \\mu \\left ( \\frac{d\\mathcal{L}_y}{d\\theta_f} - \\lambda \\frac{d\\mathcal{L}_d}{d\\theta_f} \\right ) $\n",
    "\n",
    "$\\theta_y \\leftarrow \\theta_y - \\mu\\frac{d\\mathcal{L}_y}{d\\theta_y}$\n",
    "\n",
    "$\\theta_d \\leftarrow \\theta_d - \\mu\\lambda\\frac{d\\mathcal{L}_d}{d\\theta_d}$\n",
    "\n",
    "is to simply compute loss 1, without backward compute loss 2,\n",
    "add losses with lamda scaled loss 2\n",
    "and do a simple backward\n",
    "\n",
    "**CODE**:\n",
    "\n",
    "```\n",
    "x, y = model(_x)\n",
    "l_main = lfn(y, _y)\n",
    "\n",
    "x, d = model.domain(x)\n",
    "l_dann = lfn(d, _d)\n",
    "l = l_main - ( 0.7 * l_dann)\n",
    "l.backward()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
