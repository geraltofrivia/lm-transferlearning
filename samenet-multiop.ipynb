{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# pretty print all cell's output and not just the last one\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Lib imports\n",
    "import collections\n",
    "import html\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ['QT_QPA_PLATFORM'] = 'offscreen'\n",
    "\n",
    "# FastAI Imports\n",
    "from fastai import text, core, lm_rnn\n",
    "\n",
    "# Torch imports\n",
    "import torch.nn as nn\n",
    "import torch.tensor as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Mytorch imports\n",
    "from mytorch import loops as mtlp\n",
    "from mytorch.utils.goodies import *\n",
    "from mytorch import lriters as mtlr\n",
    "\n",
    "import utils\n",
    "from options import Options as params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3c051befb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "DEBUG = True\n",
    "TRIM=False\n",
    "\n",
    "# Path fields\n",
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "\n",
    "WIKI_DATA_PATH = Path('raw/wikitext/wikitext-103/')\n",
    "WIKI_DATA_PATH.mkdir(exist_ok=True)\n",
    "IMDB_DATA_PATH = Path('raw/imdb/aclImdb/')\n",
    "IMDB_DATA_PATH.mkdir(exist_ok=True)\n",
    "PATH = Path('resources/proc/imdb')\n",
    "DATA_PROC_PATH = PATH / 'data'\n",
    "DATA_LM_PATH = PATH / 'datalm'\n",
    "\n",
    "LM_PATH = Path('resources/models')\n",
    "LM_PATH.mkdir(exist_ok=True)\n",
    "PRE_PATH = LM_PATH / 'wt103'\n",
    "PRE_LM_PATH = PRE_PATH / 'fwd_wt103.h5'\n",
    "CLASSES = ['neg', 'pos', 'unsup']\n",
    "WIKI_CLASSES = ['wiki.train.tokens', 'wiki.valid.tokens', 'wiki.test.tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network\n",
    "\n",
    "Three part network,\n",
    " - a feature extractor\n",
    " - a label predictor\n",
    " - a domain classifier\n",
    " \n",
    " Ref:\n",
    " bs = 10\n",
    " sl = 5\n",
    " hiddim = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 10\n",
    "sl = 5\n",
    "hdim = 20\n",
    "\n",
    "np_x = np.random.randint(0, 1000, (sl, bs))\n",
    "np_y = np.random.randint(0, 3, (bs))\n",
    "np_d = np.random.randint(0, 2, (bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     22,
     35,
     40,
     53
    ]
   },
   "outputs": [],
   "source": [
    "class FeatExtractor(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(1000, 20)\n",
    "        self.rnn = nn.LSTM(20, 20)\n",
    "        \n",
    "    def init_hidden(self, bs):\n",
    "        return (torch.zeros((1, bs, 20)),\n",
    "                torch.zeros((1, bs, 20)))\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        x_emb = self.emb(x)\n",
    "        print(x_emb.shape, h[0].shape, h[1].shape)\n",
    "        x, h = self.rnn(x_emb, h)\n",
    "        return x, h\n",
    "    \n",
    "    @property\n",
    "    def layers(self):\n",
    "        return torch.nn.ModuleList([\n",
    "            self.emb, self.rnn\n",
    "        ])    \n",
    "class LabelPredictor(nn.Module):\n",
    "    \n",
    "    def __init__(self, out):\n",
    "        super().__init__()\n",
    "        self.clf = nn.Linear(sl*hdim, out)\n",
    "        \n",
    "    @property\n",
    "    def layers(self):\n",
    "        return torch.nn.ModuleList([self.clf])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return x, self.clf(x)   \n",
    "class DomainClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.clf = nn.Linear(sl*hdim, out)    \n",
    "class GradReverse(Function):\n",
    "    \"\"\"\n",
    "        Torch function used to invert the sign of gradients (to be used for argmax instead of argmin)\n",
    "        Usage:\n",
    "            x = GradReverse.apply(x) where x is a tensor with grads.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg()  \n",
    "class ZeNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.f = FeatExtractor()\n",
    "        self.y = LabelPredictor(3)\n",
    "        self.d = LabelPredictor(2)\n",
    "        \n",
    "    @property\n",
    "    def layers(self):\n",
    "        return self.f.layers.extend(self.y.layers.extend(self.d.layers))\n",
    "    \n",
    "    def domain_(self, x):\n",
    "        h = self.f.init_hidden(x.shape[1])\n",
    "        x, h = self.f(x, h)\n",
    "        \n",
    "        x = x.transpose(1, 0).reshape(h[0].shape[1], -1)\n",
    "        \n",
    "        x, d = self.d(x)\n",
    "        return x, d\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.f.init_hidden(x.shape[1])\n",
    "        x, h = self.f(x, h)\n",
    "        \n",
    "        x = x.transpose(1, 0).reshape(h[0].shape[1], -1)\n",
    "        \n",
    "        x, y = self.y(x)\n",
    "#         print(y.shape)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def domain(self, x):\n",
    "        x = GradReverse.apply(x)\n",
    "        x, y = self.d(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Embedding(1000, 20)\n",
       "  (1): LSTM(20, 20)\n",
       "  (2): Linear(in_features=100, out_features=3, bias=True)\n",
       "  (3): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flow = 'dann'\n",
    "# assert flow  in ['main', 'dann', 'both']\n",
    "\n",
    "model = ZeNetwork()\n",
    "model.layers\n",
    "lfn = nn.CrossEntropyLoss()\n",
    "_x = torch.tensor(np_x)\n",
    "_y = torch.tensor(np_y)\n",
    "_d = torch.tensor(np_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 2, 0, 2, 0, 2, 2, 0, 0]),\n",
       " tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_y, _d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 20]) torch.Size([1, 10, 20]) torch.Size([1, 10, 20])\n",
      "tensor(1.1052, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "x, y = model(_x)\n",
    "l_main = lfn(y, _y)\n",
    "l_main.backward()\n",
    "grads_main_f = [param.grad.clone() for param in model.f.rnn.parameters()]\n",
    "print(l_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 20]) torch.Size([1, 10, 20]) torch.Size([1, 10, 20])\n",
      "tensor(0.6961, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "x, d = model.domain_(_x)\n",
    "l_dann = lfn(d, _d)\n",
    "l_dann.backward()\n",
    "grads_dann_f = [param.grad.clone() for param in model.f.rnn.parameters()]\n",
    "print(l_dann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 20]) torch.Size([1, 10, 20]) torch.Size([1, 10, 20])\n"
     ]
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "_x = torch.tensor(np_x)\n",
    "_y = torch.tensor(np_y)\n",
    "_d = torch.tensor(np_d)\n",
    "\n",
    "x, y = model(_x)\n",
    "l_main = lfn(y, _y)\n",
    "# l_main.backward(retain_graph=True)\n",
    "\n",
    "x, d = model.domain(x)\n",
    "l_dann = lfn(d, _d)\n",
    "l = l_main - ( 0.7 * l_dann)\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0024,  0.0023, -0.0031,  ..., -0.0015, -0.0019, -0.0030],\n",
       "        [ 0.0003, -0.0003,  0.0010,  ...,  0.0008, -0.0002, -0.0016],\n",
       "        [ 0.0030,  0.0003, -0.0005,  ..., -0.0026,  0.0018,  0.0007],\n",
       "        ...,\n",
       "        [-0.0010,  0.0022, -0.0008,  ...,  0.0027, -0.0015, -0.0003],\n",
       "        [ 0.0002,  0.0017,  0.0012,  ...,  0.0007,  0.0012,  0.0007],\n",
       "        [ 0.0015,  0.0037,  0.0017,  ..., -0.0001, -0.0002,  0.0006]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads_main_f[0] + (0.7*grads_dann_f[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0024,  0.0023, -0.0031,  ..., -0.0015, -0.0019, -0.0030],\n",
       "        [ 0.0003, -0.0003,  0.0010,  ...,  0.0008, -0.0002, -0.0016],\n",
       "        [ 0.0030,  0.0003, -0.0005,  ..., -0.0026,  0.0018,  0.0007],\n",
       "        ...,\n",
       "        [-0.0010,  0.0022, -0.0008,  ...,  0.0027, -0.0015, -0.0003],\n",
       "        [ 0.0002,  0.0017,  0.0012,  ...,  0.0007,  0.0012,  0.0007],\n",
       "        [ 0.0015,  0.0037,  0.0017,  ..., -0.0001, -0.0002,  0.0006]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[param.grad for param in model.f.rnn.parameters()][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion!\n",
    "\n",
    "What I find is, that the best way to do the DANN updates, i.e.\n",
    "\n",
    "$ \\theta_f \\leftarrow \\theta_f - \\mu \\left ( \\frac{d\\mathcal{L}_y}{d\\theta_f} - \\lambda \\frac{d\\mathcal{L}_d}{d\\theta_f} \\right ) $\n",
    "\n",
    "$\\theta_y \\leftarrow \\theta_y - \\mu\\frac{d\\mathcal{L}_y}{d\\theta_y}$\n",
    "\n",
    "$\\theta_d \\leftarrow \\theta_d - \\mu\\lambda\\frac{d\\mathcal{L}_d}{d\\theta_d}$\n",
    "\n",
    "is to simply compute loss 1, without backward compute loss 2,\n",
    "add losses with lamda scaled loss 2\n",
    "and do a simple backward\n",
    "\n",
    "**CODE**:\n",
    "\n",
    "```\n",
    "x, y = model(_x)\n",
    "l_main = lfn(y, _y)\n",
    "\n",
    "x, d = model.domain(x)\n",
    "l_dann = lfn(d, _d)\n",
    "l = l_main - ( 0.7 * l_dann)\n",
    "l.backward()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
