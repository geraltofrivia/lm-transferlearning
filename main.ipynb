{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-24T10:20:20.495317Z",
          "start_time": "2019-03-24T10:20:20.491681Z"
        },
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "\n",
        "# pretty print all cell\u0027s output and not just the last one\n",
        "InteractiveShell.ast_node_interactivity \u003d \"all\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-24T10:20:24.914252Z",
          "start_time": "2019-03-24T10:20:22.767793Z"
        },
        "code_folding": [
          0.0
        ],
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "from functools import partial\n",
        "from typing import Tuple, Union, Callable\n",
        "\n",
        "# Torch imports\n",
        "import torch.nn as nn\n",
        "import torch.tensor as tensor\n",
        "import torch.nn.functional as func\n",
        "\n",
        "# Mytorch imports\n",
        "from mytorch import loops as mtlp\n",
        "from mytorch.utils.goodies import *\n",
        "from mytorch import lriters as mtlr\n",
        "\n",
        "# Local imports\n",
        "from utils import dann_loop\n",
        "from data import DataPuller\n",
        "from options import Phase2 as params\n",
        "\n",
        "os.environ[\u0027QT_QPA_PLATFORM\u0027] \u003d \u0027offscreen\u0027\n",
        "\n",
        "# FastAI Imports\n",
        "from fastai import text, core, lm_rnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-24T10:20:42.833584Z",
          "start_time": "2019-03-24T10:20:42.824664Z"
        },
        "pycharm": {}
      },
      "outputs": [],
      "source": "QUICK \u003d True\nDEBUG \u003d True\nPRETRAINED \u003d True\nMESSAGE \u003d \"A new start\"\nSAFE_MODE \u003d True\nDATASETS \u003d \"imdb,wikitext\".split(\u0027,\u0027)\n\nif len(DATASETS) \u003c 2: params.loss_scale \u003d 0.0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [
          17.0,
          22.0,
          87.0,
          92.0,
          101.0
        ],
        "pycharm": {}
      },
      "outputs": [],
      "source": "\nDEVICE \u003d \u0027cuda\u0027\nKNOWN_DATASETS \u003d [\u0027imdb\u0027, \u0027wikitext\u0027, \u0027\u0027]\n\ndevice \u003d torch.device(DEVICE)\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Path fields\nPATH \u003d Path(\u0027resources/proc/imdb\u0027)\nDATA_PROC_PATH \u003d PATH / \u0027data\u0027\nDATA_LM_PATH \u003d PATH / \u0027datalm\u0027\nDUMP_PATH \u003d Path(\u0027resources/models/runs\u0027)\n\nLM_PATH \u003d Path(\u0027resources/models\u0027)\nLM_PATH.mkdir(exist_ok\u003dTrue)\nPRE_PATH \u003d LM_PATH / \u0027wt103\u0027\nPRE_LM_PATH \u003d PRE_PATH / \u0027fwd_wt103.h5\u0027\n\n\u0027\u0027\u0027\n    Data sampler for this training\n\u0027\u0027\u0027\n\n\n# noinspection PyShadowingNames\nclass DomainAgnosticSampler:\n    \"\"\" Sample data for language model training from two different domains in one batch. \"\"\"\n\n    def __init__(self, data: Tuple[Union[list, np.ndarray], Union[list, np.ndarray]], data_fn: Callable):\n        \"\"\"\n            Here, data_fn would be something like\n                `partial(text.LanguageModelLoader, bs\u003dbs, bptt\u003dbptt)`\n            And data_a/b would be something like\n                `{\u0027train\u0027: np.concatenate(trn_lm), \u0027valid\u0027: np.concatenate(val_lm)}[\u0027train\u0027]`\n            data_fn (fastai\u0027s language model loader) flattens y and returns x of seqlen, batchsize\n        \"\"\"\n        self.args \u003d {\u0027data_fn\u0027: data_fn, \u0027data\u0027: data,}\n        self.iters \u003d [iter([]) for _ in range(len(data))]\n        self.reset(**self.args)\n\n    def reset(self, data_fn: Callable, data: list):\n        self.iters \u003d [iter(data_fn(data_)) for data_ in data]\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        x,y \u003d [], []\n        for iter_ in self.iters:\n            x_, y_ \u003d iter_.__next__()\n            x.append(x_)\n            y.append(y_)\n        return self._combine_batch_(x, y)\n\n    def __len__(self):\n        return min([len(self.args[\u0027data_fn\u0027](data_)) for data_ in self.args[\u0027data\u0027] ])\n\n    @staticmethod\n    def _combine_batch_(x, y):\n        \"\"\"\n            :param x is a list of np.arr looks like seqlen, batchsize\n            :param y is a corresponding list of np.arr (one word ahead than x_a) which is a flattened x_a.shape mat\n\n             Returns x, y, y_dom in similar shapes as input\n        \"\"\"\n\n        # Get them to interpretable shapes\n        y \u003d [y_.reshape(x[i].shape).transpose(1, 0) for i,y_ in enumerate(y)]\n        x \u003d [x_.transpose(1, 0) for x_ in x]\n\n        b_bs, b_sl \u003d x[0].shape[0], min([x_.shape[1] for x_ in x])\n\n        # Concatenate to make an x and y\n        x \u003d np.concatenate([x_[:, :b_sl] for x_ in x])\n        y \u003d np.concatenate([y_[:, :b_sl] for y_ in y])\n\n        # Shuffle and remember shuffle index to make y labels for domain agnostic training\n        intrp \u003d np.arange(b_bs * 2)\n        np.random.shuffle(intrp)\n        y_dom \u003d (intrp \u003e\u003d b_bs) * 1\n        x \u003d x[intrp]\n        y \u003d y[intrp]\n\n        x \u003d x.transpose(1, 0)\n        y \u003d y.transpose(1, 0).reshape(np.prod(y.shape))\n\n        return x, y, y_dom\n\n\n\u0027\u0027\u0027\n    Model definitions\n\u0027\u0027\u0027\n\n\nclass CustomEncoder(lm_rnn.RNN_Encoder):\n\n    def forward(self, input, domain\u003dNone):\n        \"\"\" Overwrote fn to keep the interface same b/w phase 2 \u0026 phase 3 models (same training loop)\"\"\"\n        return super().forward(input)\n\n    @property\n    def layers(self):\n        return torch.nn.ModuleList([torch.nn.ModuleList([self.rnns[0], self.dropouths[0]]),\n                                    torch.nn.ModuleList([self.rnns[1], self.dropouths[1]]),\n                                    torch.nn.ModuleList([self.rnns[2], self.dropouths[2]])])\n\n\nclass CustomDecoder(text.LinearDecoder):\n\n    @property\n    def layers(self):\n        return torch.nn.ModuleList([self.decoder, self.dropout])\n\n    def forward(self, x):\n        raw_outputs, outputs \u003d x\n        output \u003d self.dropout(outputs[-1])\n        decoded \u003d self.decoder(output.view(output.size(0) * output.size(1), output.size(2)))\n        result \u003d decoded.view(-1, decoded.size(1))\n        return result, (raw_outputs, outputs)\n\n\n# noinspection PyShadowingNames\nclass CustomLinear(lm_rnn.PoolingLinearClassifier):\n\n    def forward(self, x):\n        raw_outputs, outputs \u003d x\n        output \u003d outputs[-1]\n        sl,bs,_ \u003d output.size()\n        avgpool \u003d self.pool(output, bs, False)\n        mxpool \u003d self.pool(output, bs, True)\n        x \u003d torch.cat([output[-1], mxpool, avgpool], 1)\n        for i, l in enumerate(self.layers):\n            l_x \u003d l(x)\n            if i !\u003d len(self.layers) - 1:\n                x \u003d func.relu(l_x)\n            else:\n                x \u003d torch.sigmoid(l_x)\n        # noinspection PyUnboundLocalVariable\n        return l_x, (raw_outputs, outputs)\n\n\nclass LanguageModel(nn.Module):\n\n    def __init__(self,\n                 _parameter_dict,\n                 _device,\n                 _encargs,\n                 _n_tasks\u003d2,\n                 _wgts_e\u003dNone,\n                 _wgts_d\u003dNone):\n        super(LanguageModel, self).__init__()\n\n        self.parameter_dict \u003d _parameter_dict\n        self.device \u003d _device\n\n        self.encoder \u003d CustomEncoder(**_encargs).to(self.device)\n        if _wgts_e:\n            self.encoder.load_state_dict(_wgts_e)\n        \"\"\"\n            Explanation:\n                400*3 because input is [ h_T, maxpool, meanpool ]\n                0.4, 0.1 are drops at various layersLM_PATH\n        \"\"\"\n        self.linear_dec \u003d CustomDecoder(\n            _encargs[\u0027ntoken\u0027],\n            n_hid\u003d400,\n            dropout\u003dparams.decoder_drops,\n            tie_encoder\u003dself.encoder.encoder,\n            bias\u003dFalse\n        ).to(self.device)\n\n        self.linear_dom \u003d CustomLinear(layers\u003dparams.domclas_layers + [_n_tasks], drops\u003dparams.domclas_drops).to(self.device)\n        self.encoder.reset()\n\n    def forward(self, x, d):\n        \"\"\" d is not used (only so the loop remains same b/w phase 2 and phase 3 models) \"\"\"\n        x_enc \u003d self.encoder(x, d)\n        return self.linear_dec(x_enc)\n\n    def domain(self, x_enc):\n        x_enc \u003d list(x_enc)\n        x_enc[1] \u003d [GradReverse.apply(enc_tensr) for enc_tensr in x_enc[1]]\n        return self.linear_dom(x_enc)[0]\n\n    @property\n    def layers(self):\n        return self.encoder.layers.extend(self.linear_dec.layers).extend(self.linear_dom.layers)\n\n    def predict(self, x, d):\n        with torch.no_grad():\n            self.eval()\n            pred \u003d self.forward(x, d)\n            self.train()\n            return pred\n\n\ndef _eval(y_pred, y_true, tasks: int\u003d1, task_index: torch.tensor\u003dNone):\n    \"\"\"\n        Expects a batch of input\n\n        :param y_pred: tensor of shape (b, nc)\n        :param y_true: tensor of shape (b, 1)\n    \"\"\"\n    return torch.mean((torch.argmax(y_pred, dim\u003d1) \u003d\u003d y_true).float())\n\n\ndef loss_wrapper(y_pred, y_true, loss_fn, **args):\n    return loss_fn(y_pred, y_true)\n\n\nclass CustomLanguageModelLoader(text.LanguageModelLoader):\n    \"\"\" Overwriting the class so we can call it within the same way of iterating over data as in other cases.\"\"\"\n\n    def __init__(self, data, **args):\n        super().__init__(data, **args)\n        \n        \n    def __iter__(self):\n        self.i, self.iter \u003d 0, 0\n        while self.i \u003c self.n - 1 and self.iter \u003c len(self):\n            if self.i \u003d\u003d 0:\n                seq_len \u003d self.bptt + 5 * 5\n            else:\n                bptt \u003d self.bptt if np.random.random() \u003c 0.95 else self.bptt / 2.\n                seq_len \u003d max(5, int(np.random.normal(bptt, 5)))\n            res \u003d self.get_batch(self.i, seq_len)\n            _res \u003d list(res) + [torch.zeros(res[0].shape[1])]\n            self.i +\u003d seq_len\n            self.iter +\u003d 1\n            yield res\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "# Check args.\nif DATASETS \u003d\u003d [\u0027\u0027]:\n    DATASETS \u003d []\nelse:\n    for dataset in DATASETS:\n        assert dataset in KNOWN_DATASETS, f\"Couldn\u0027t find a dataset called {dataset}. Exiting.\"\n\nparams.message \u003d MESSAGE\nparams.quick \u003d QUICK\nparams.datasets \u003d DATASETS\n\nif DEBUG:\n    print(\"Pulling data from disk\")\n\n# Pulling data from disk\ndata_puller \u003d DataPuller(debug\u003dFalse, max_vocab\u003dparams.max_vocab_task, min_freq\u003dparams.min_vocab_freq, trim_trn\u003d1000, trim_val\u003d-1)\n\ntrn_lm, val_lm \u003d [], []\nfor dataset in DATASETS:\n\n    trn_lm_, val_lm_, itos \u003d data_puller.get(dataset, supervised\u003dFalse, trim\u003dparams.quick, cached\u003dTrue, merge_vocab\u003dparams.max_vocab_others)\n\n    # Append data to main lists\n    trn_lm.append(trn_lm_)\n    val_lm.append(val_lm_)\n\nvs \u003d len(itos)\n\n\"\"\"\n    Now we pull pretrained models from disk    \n\"\"\"\n\nif DEBUG:\n    print(\"Pulling models from disk\")\n\nem_sz, nh, nl \u003d 400, 1150, 3\nwgts \u003d torch.load(PRE_LM_PATH, map_location\u003dlambda storage, loc: storage)\nenc_wgts \u003d core.to_np(wgts[\u00270.encoder.weight\u0027])\nrow_m \u003d enc_wgts.mean(0)\nitos2 \u003d pickle.load((PRE_PATH / \u0027itos_wt103.pkl\u0027).open(\u0027rb\u0027))\nstoi2 \u003d collections.defaultdict(lambda: -1, {v: k for k, v in enumerate(itos2)})\nnew_w \u003d np.zeros((vs, em_sz), dtype\u003dnp.float32)\nfor i, w in enumerate(itos):\n    r \u003d stoi2[w]\n    new_w[i] \u003d enc_wgts[r] if r \u003e\u003d 0 else row_m\n\n# noinspection PyCallingNonCallable\nwgts[\u00270.encoder.weight\u0027] \u003d tensor(new_w)\n# noinspection PyCallingNonCallable\nwgts[\u00270.encoder_with_dropout.embed.weight\u0027] \u003d tensor(np.copy(new_w))\n# noinspection PyCallingNonCallable\nwgts[\u00271.decoder.weight\u0027] \u003d tensor(np.copy(new_w))\nwgts_enc \u003d {\u0027.\u0027.join(k.split(\u0027.\u0027)[1:]): val\n            for k, val in wgts.items() if k[0] \u003d\u003d \u00270\u0027}\nwgts_dec \u003d {\u0027.\u0027.join(k.split(\u0027.\u0027)[1:]): val\n            for k, val in wgts.items() if k[0] \u003d\u003d \u00271\u0027}\n\n\u0027\u0027\u0027\n    Setting up things for training.\n\u0027\u0027\u0027\nbptt \u003d 70\nbs \u003d params.bs\nopt_fn \u003d partial(torch.optim.SGD)  # , betas\u003dparams.adam_betas)\nlengths \u003d np.array([len(CustomLanguageModelLoader(np.concatenate(trn_lm_), bs\u003dbs, bptt\u003dbptt)) for trn_lm_ in trn_lm])\n# l_a, l_b \u003d len(text.LanguageModelLoader(np.concatenate(trn_lm), bs\u003dbs, bptt\u003dbptt)), \\\n#            len(text.LanguageModelLoader(np.concatenate(wiki_trn_lm), bs\u003dbs, bptt\u003dbptt))\nweights \u003d torch.tensor(np.ascontiguousarray((lengths/np.sum(lengths))[::-1]), dtype\u003dtorch.float, device\u003ddevice) if len(DATASETS) \u003e 1 else None\n\n# Load the pre-trained model\nparameter_dict \u003d {\u0027itos2\u0027: itos2}\ndps \u003d params.encoder_drops\nencargs \u003d {\u0027ntoken\u0027: new_w.shape[0],\n           \u0027emb_sz\u0027: 400, \u0027n_hid\u0027: 1150,\n           \u0027n_layers\u0027: 3, \u0027pad_token\u0027: 0,\n           \u0027qrnn\u0027: False, \u0027dropouti\u0027: dps[0],\n           \u0027wdrop\u0027: dps[2], \u0027dropoute\u0027: dps[3], \u0027dropouth\u0027: dps[4]}\n\nlm \u003d LanguageModel(parameter_dict, device, _encargs\u003dencargs, _n_tasks\u003dlen(DATASETS),\n                   _wgts_e\u003dwgts_enc if PRETRAINED else None, _wgts_d\u003dwgts_dec)\nopt \u003d make_opt(lm, opt_fn, lr\u003dparams.lr.init)\nloss_main_fn \u003d partial(loss_wrapper, loss_fn\u003dfunc.cross_entropy)\nloss_aux_fn \u003d partial(loss_wrapper, loss_fn\u003dnn.CrossEntropyLoss(weights))\n\n# Make data\nif len(DATASETS) \u003e 1:\n    data_fn_unidomain \u003d partial(text.LanguageModelLoader, bs\u003dbs, bptt\u003dbptt)\n    data_train \u003d [np.concatenate(trn_lm_) for trn_lm_ in trn_lm]\n    data_valid \u003d [np.concatenate(val_lm_) for val_lm_ in val_lm]\n    data \u003d {\u0027train\u0027: data_train, \u0027valid\u0027: data_valid}\n    data_fn \u003d partial(DomainAgnosticSampler, data_fn\u003ddata_fn_unidomain)\nelif len(DATASETS) \u003d\u003d 1:\n    data_fn_unidomain \u003d partial(CustomLanguageModelLoader, bs\u003dbs, bptt\u003dbptt)\n    data_train \u003d [np.concatenate(trn_lm_) for trn_lm_ in trn_lm][0]\n    data_valid \u003d [np.concatenate(val_lm_) for val_lm_ in val_lm][0]\n    data \u003d {\u0027train\u0027: data_train, \u0027valid\u0027: data_valid}\n    data_fn \u003d data_fn_unidomain\nelse:\n    # No dataset\n    data_fn_unidomain, data_train, data_valid, data, data_fn \u003d [None] * 5\n\n# Set up lr and freeze stuff\nfor grp in opt.param_groups:\n    grp[\u0027lr\u0027] \u003d 0.0\nopt.param_groups[3][\u0027lr\u0027] \u003d params.lr.init\nopt.param_groups[4][\u0027lr\u0027] \u003d params.lr.init\n\n# lr_args \u003d {\u0027batches\u0027:, \u0027cycles\u0027: 1}\nif len(DATASETS) \u003e 0:\n    lr_args \u003d {\u0027iterations\u0027: len(data_fn(data\u003ddata[\u0027train\u0027])),\n               \u0027cut_frac\u0027: params.lr.sltr_cutfrac, \u0027ratio\u0027: params.lr.sltr_ratio}\n    lr_schedule \u003d mtlr.LearningRateScheduler(optimizer\u003dopt, lr_args\u003dlr_args, lr_iterator\u003dmtlr.SlantedTriangularLR)\n    \n# Find places to save model\nsave_dir \u003d mt_save_dir(DUMP_PATH / \u0027_\u0027.join(DATASETS), _newdir\u003dTrue) if not SAFE_MODE else \u0027\u0027\nsave_fnames \u003d {\u0027torch_stuff\u0027:\n                   {\u0027hightrn\u0027:\n                        {\u0027model\u0027: \u0027unsup_model_hightrn.torch\u0027,\n                         \u0027enc\u0027: \u0027unsup_model_hightrn_enc.torch\u0027},\n                    \u0027lowaux\u0027:\n                        {\u0027model\u0027: \u0027unsup_model_lowaux.torch\u0027,\n                         \u0027enc\u0027: \u0027unsup_model_lowaux_enc.torch\u0027}}}\n\nif not SAFE_MODE:\n    # Start to put permanent things there, like the itos\n    mt_save(save_dir,\n            pickle_stuff\u003d[tosave(\u0027itos.pkl\u0027, itos)])\n    \nif len(DATASETS) \u003d\u003d 0:\n\n    if not SAFE_MODE:\n        # Dump the model and vocab as it is!\n        mt_save(save_dir, message\u003dMESSAGE,\n                torch_stuff\u003d[tosave(\u0027unsup_model_final.torch\u0027, lm.state_dict()),\n                             tosave(\u0027unsup_model_enc_final.torch\u0027, lm.encoder.state_dict())],\n                pickle_stuff\u003d[tosave(\u0027itos.pkl\u0027, itos), tosave(\u0027unsup_options.pkl\u0027, params)])\n\n    # Nothing more to do. Quit.\n    warnings.warn(\"No dataset specified. Dumped the model, and vocab. Quitting the code now. Tschuss!\")\n    exit()\n\n\nargs \u003d {\u0027epochs\u0027: 1, \u0027weight_decay\u0027: params.weight_decay, \u0027data\u0027: data,\n        \u0027device\u0027: device, \u0027opt\u0027: opt, \u0027loss_main_fn\u0027: loss_main_fn, \u0027loss_aux_fn\u0027: loss_aux_fn,\n        \u0027train_fn\u0027: lm, \u0027train_aux_fn\u0027: lm.domain, \u0027predict_fn\u0027: lm.predict, \u0027data_fn\u0027: data_fn, \u0027model\u0027: lm,\n        \u0027eval_fn\u0027: _eval, \u0027eval_aux_fn\u0027: _eval, \u0027batch_start_hook\u0027: partial(mtlp.reset_hidden, lm), \u0027tasks\u0027: 2,\n        \u0027clip_grads_at\u0027: params.clip_grads_at, \u0027lr_schedule\u0027: lr_schedule, \u0027loss_aux_scale\u0027: params.loss_scale,\n        \u0027save_dir\u0027: save_dir, \u0027save\u0027: not SAFE_MODE, \u0027save_params\u0027: params, \u0027save_fnames\u0027: save_fnames}"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "pycharm": {}
      },
      "outputs": [],
      "source": "\u0027\u0027\u0027\n    Actual training\n\u0027\u0027\u0027\n# print(\"Time taken to get everything so far done\")\ntraces_start \u003d dann_loop(**args)\n\n# Now unfreeze all layers and apply discr\nfor grp in opt.param_groups:\n    grp[\u0027lr\u0027] \u003d params.lr.init\n\nlr_dscr \u003d lambda optim, lr, fctr\u003dparams.lr.dscr: [lr / (fctr ** p_grp) for p_grp in range(len(optim.param_groups))[::-1]]\nupdate_lr(opt, lr_dscr(opt, params.lr.init))\n\nif DEBUG:\n    print([x[\u0027lr\u0027] for x in opt.param_groups])\n\nlr_args \u003d {\u0027iterations\u0027: len(data_fn(data\u003ddata[\u0027train\u0027])) * 15,\n           \u0027cut_frac\u0027: params.lr.sltr_cutfrac, \u0027ratio\u0027: params.lr.sltr_ratio}\nlr_schedule \u003d mtlr.LearningRateScheduler(optimizer\u003dopt, lr_args\u003dlr_args, lr_iterator\u003dmtlr.SlantedTriangularLR)\nargs[\u0027save_above_trn\u0027] \u003d np.max(traces_start[0])\n# args[\u0027save_above_aux\u0027] \u003d np.min(traces_start[2][2:])  # Not updating this var since we ignore the DANN acc of the first few epochs anyway\nargs[\u0027lr_schedule\u0027] \u003d lr_schedule\nargs[\u0027epochs\u0027] \u003d 15\nargs[\u0027epoch_count\u0027] \u003d 1\nargs[\u0027notify\u0027] \u003d True\n\ntraces_main \u003d dann_loop(**args)\ntraces \u003d [a + b for a, b in zip(traces_start, traces_main)]"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [],
        "pycharm": {}
      },
      "outputs": [],
      "source": "from matplotlib import pyplot as plt\nfrom matplotlib import style as pltstyle\n# import matplotlib.style as style\n# style.available\n%pylab inline\n# pylab.rcParams[\u0027figure.figsize\u0027] \u003d (16, 8)\n\n\ndef plot_accs(tra, vla, axa, style\u003dNone, _savedir\u003dNone):\n    pltstyle.use(style if style else \u0027seaborn-deep\u0027)\n    fig \u003d plt.figure(figsize \u003d (16,8))\n    ax \u003d fig.add_axes((0.1, 0.2, 0.8, 0.7))\n    ax.spines[\u0027right\u0027].set_color(\u0027none\u0027)\n    ax.spines[\u0027top\u0027].set_color(\u0027none\u0027)\n#     plt.xticks([])\n#     plt.yticks([])\n    ax.plot(tra, label\u003df\"Train Acc\", linewidth\u003d3)\n    ax.plot(vla, label\u003df\"Valid Acc\", linewidth\u003d3)\n    ax.plot(axa, label\u003df\"Aux Acc\", linewidth\u003d3)\n    ax.set_ylim(bottom\u003d0)\n    ax.legend()\n    plt.show()\n    \n    if _savedir:\n        pltstyle.use(style if style else \u0027seaborn-deep\u0027)\n        fig \u003d plt.figure(figsize \u003d (16,8))\n        ax \u003d fig.add_axes((0.1, 0.2, 0.8, 0.7))\n        ax.spines[\u0027right\u0027].set_color(\u0027none\u0027)\n        ax.spines[\u0027top\u0027].set_color(\u0027none\u0027)\n    #     plt.xticks([])\n    #     plt.yticks([])\n        ax.plot(tra, label\u003df\"Train Acc\", linewidth\u003d3)\n        ax.plot(vla, label\u003df\"Valid Acc\", linewidth\u003d3)\n        ax.plot(axa, label\u003df\"Aux Acc\", linewidth\u003d3)\n        ax.set_ylim(bottom\u003d0)\n        ax.legend()\n        plt.show()\n    \n    \ndef plot(trcs):\n    layers \u003d len(trcs[0])\n    for l in range(layers):\n        plt.plot(trcs[:,l], label\u003df\"layer {l}\")\n    plt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "plot_accs(traces[0], traces[1], traces[2], style\u003d\u0027seaborn-poster\u0027, _savedir\u003dsave_dir/\u0027unsup_acc.png\u0027 if not SAFE_MODE else None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "plot(np.asarray(traces[-1][100:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Dumping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": "# Dumping stuff\nif not SAFE_MODE:\n    mt_save(save_dir, message\u003dMESSAGE,\n            torch_stuff\u003d[tosave(\u0027unsup_model_final.torch\u0027, lm.state_dict()),\n                         tosave(\u0027unsup_model_enc_final.torch\u0027, lm.encoder.state_dict())],\n            pickle_stuff\u003d[tosave(\u0027final_unsup_traces.pkl\u0027, traces), tosave(\u0027unsup_options.pkl\u0027, params)])"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "save_dir\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Interpreting Traces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "trn_best \u003d np.max(traces[0])\n",
        "trn_best_ \u003d np.argmax(traces[0])\n",
        "val_attrn \u003d traces[1][trn_best_]\n",
        "val_best \u003d np.max(traces[1])\n",
        "val_best_ \u003d np.argmax(traces[1])\n",
        "aux_attrn \u003d traces[2][trn_best_]\n",
        "aux_best \u003d np.min(traces[2][2:])\n",
        "aux_best_ \u003d np.argmin(traces[2][2:])\n",
        "print(f\"Train Best: {trn_best:.4f} at {trn_best_}\\nValid @Trn: {val_attrn:.4f}\\nValid Best: {val_best:.4f} at {val_best_}\\nDomAg @Trn: {aux_attrn:.4f}\\nDomAg Best: {aux_best:.4f} at {aux_best_}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}