{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# pretty print all cell's output and not just the last one\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0c18a42270>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# External Lib imports\n",
    "import re\n",
    "import html\n",
    "import pickle\n",
    "import sklearn\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from functools import partial\n",
    "from typing import AnyStr, Callable\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "os.environ['QT_QPA_PLATFORM']='offscreen'\n",
    "\n",
    "# FastAI Imports\n",
    "from fastai import text, core, lm_rnn\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.tensor as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Mytorch imports\n",
    "from mytorch import loops, lriters\n",
    "from mytorch.utils.goodies import *\n",
    "\n",
    "device = torch.device('cuda')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "\n",
    "# Path fields\n",
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "\n",
    "WIKI_DATA_PATH = Path('raw/wikitext/wikitext-103/')\n",
    "WIKI_DATA_PATH.mkdir(exist_ok=True)\n",
    "IMDB_DATA_PATH = Path('raw/imdb/aclImdb/')\n",
    "IMDB_DATA_PATH.mkdir(exist_ok=True)\n",
    "PATH = Path('resources/proc/imdb')\n",
    "DATA_PROC_PATH = PATH / 'data'\n",
    "DATA_LM_PATH = PATH / 'datalm'\n",
    "\n",
    "LM_PATH = Path('resources/models')\n",
    "LM_PATH.mkdir(exist_ok=True)\n",
    "PRE_PATH = LM_PATH / 'wt103'\n",
    "PRE_LM_PATH = PRE_PATH / 'fwd_wt103.h5'\n",
    "CLASSES = ['neg', 'pos', 'unsup']\n",
    "WIKI_CLASSES = ['wiki.train.tokens', 'wiki.valid.tokens', 'wiki.test.tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data from Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75000 25000\n"
     ]
    }
   ],
   "source": [
    "def get_texts_org(path):\n",
    "    texts, labels = [], []\n",
    "    for idx, label in enumerate(CLASSES):\n",
    "        for fname in (path / label).glob('*.*'):\n",
    "            texts.append(fname.open('r', encoding='utf-8').read())\n",
    "            labels.append(idx)\n",
    "    return np.array(texts), np.array(labels)\n",
    "\n",
    "trn_texts, trn_labels = get_texts_org(IMDB_DATA_PATH / 'train')\n",
    "val_texts, val_labels = get_texts_org(IMDB_DATA_PATH / 'test')\n",
    "col_names = ['labels', 'text']\n",
    "print(len(trn_texts), len(val_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     5
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "859955 1841\n"
     ]
    }
   ],
   "source": [
    "def is_valid_sent(x):\n",
    "    x = x.strip()\n",
    "    if len(x) == 0: return False\n",
    "    if x[0] == '=' and x[-1] == '=': return False\n",
    "    return True\n",
    "def wiki_get_texts_org(path):\n",
    "    texts = []\n",
    "    for idx, label in enumerate(WIKI_CLASSES):\n",
    "        with open(path / label, encoding='utf-8') as f:\n",
    "            texts.append([sent.strip() for sent in f.readlines() if is_valid_sent(sent)])\n",
    "    return tuple(texts)\n",
    "wiki_trn_texts, wiki_val_texts, wiki_tst_texts = wiki_get_texts_org(WIKI_DATA_PATH)\n",
    "print(len(wiki_trn_texts), len(wiki_val_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_texts, val_texts = trn_texts[:1000], val_texts[:1000]\n",
    "wiki_trn_texts, wiki_val_texts, wiki_tst_texts = wiki_trn_texts[:1000], wiki_val_texts[:1000], wiki_tst_texts[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "trn_idx = np.random.permutation(len(trn_texts))\n",
    "val_idx = np.random.permutation(len(val_texts))\n",
    "\n",
    "trn_texts, trn_labels = trn_texts[trn_idx], trn_labels[trn_idx]\n",
    "val_texts, val_labels = val_texts[val_idx], val_labels[val_idx]\n",
    "\n",
    "# Shuffle data (wiki)\n",
    "np.random.shuffle(wiki_trn_texts)\n",
    "np.random.shuffle(wiki_val_texts)\n",
    "np.random.shuffle(wiki_tst_texts)\n",
    "\n",
    "wiki_trn_labels = [0 for _ in wiki_trn_texts]\n",
    "wiki_val_labels = [0 for _ in wiki_val_texts]\n",
    "wiki_tst_labels = [0 for _ in wiki_val_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe black magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     2,
     8,
     16,
     22
    ]
   },
   "outputs": [],
   "source": [
    "chunksize = 24000\n",
    "re1 = re.compile(r'  +')\n",
    "def _fixup_(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
    "        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "def _get_texts_(df, n_lbls=1):\n",
    "    labels = df.iloc[:, range(n_lbls)].values.astype(np.int64)\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)\n",
    "    for i in range(n_lbls + 1, len(df.columns)): texts += f' {FLD} {i-n_lbls} ' + df[i].astype(str)\n",
    "    texts = list(texts.apply(_fixup_).values)\n",
    "\n",
    "    tok = text.Tokenizer().proc_all_mp(core.partition_by_cores(texts))\n",
    "    return tok, list(labels)\n",
    "def _simple_apply_fixup_(df):\n",
    "    labels = [0] * df.shape[0]\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df.text\n",
    "    texts = list(texts.apply(_fixup_).values)\n",
    "    tok = text.Tokenizer().proc_all_mp(core.partition_by_cores(texts))\n",
    "    return tok, list(labels)   \n",
    "def get_all(df, n_lbls):\n",
    "    tok, labels = [], []\n",
    "    for i, r in enumerate(df):\n",
    "        print(i)\n",
    "        tok_, labels_ = _get_texts_(r, n_lbls)\n",
    "        tok += tok_;\n",
    "        labels += labels_\n",
    "    return tok, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800 200\n",
      "Trn: (1800, 1800), Val: (200, 200) \n"
     ]
    }
   ],
   "source": [
    "trn_texts, val_texts = sklearn.model_selection.train_test_split(\n",
    "        np.concatenate([trn_texts, val_texts]), test_size=0.1)\n",
    "\n",
    "if DEBUG:\n",
    "    print(len(trn_texts), len(val_texts))\n",
    "\n",
    "df_trn = pd.DataFrame({'text': trn_texts, 'labels': [0] * len(trn_texts)}, columns=col_names)\n",
    "df_val = pd.DataFrame({'text': val_texts, 'labels': [0] * len(val_texts)}, columns=col_names)\n",
    "\n",
    "trn_tok, trn_labels = _simple_apply_fixup_(df_trn)\n",
    "val_tok, val_labels = _simple_apply_fixup_(df_val)\n",
    "\n",
    "if DEBUG:\n",
    "    print(f\"Trn: {len(trn_tok), len(trn_labels)}, Val: {len(val_tok), len(val_labels)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700 300\n",
      "Trn: (2700, 2700), Val: (300, 300) \n"
     ]
    }
   ],
   "source": [
    "wiki_trn_texts, wiki_val_texts = sklearn.model_selection.train_test_split(\n",
    "        np.concatenate([wiki_trn_texts, wiki_val_texts, wiki_tst_texts]), test_size=0.1)\n",
    "\n",
    "if DEBUG:\n",
    "    print(len(wiki_trn_texts), len(wiki_val_texts))\n",
    "    \n",
    "wiki_df_trn = pd.DataFrame({'text':wiki_trn_texts, 'labels': [0] * len(wiki_trn_texts)}, columns=col_names)\n",
    "wiki_df_val = pd.DataFrame({'text':wiki_val_texts, 'labels': [0] * len(wiki_val_texts)}, columns=col_names)\n",
    "\n",
    "wiki_trn_tok, wiki_trn_labels = _simple_apply_fixup_(wiki_df_trn)\n",
    "wiki_val_tok, wiki_val_labels = _simple_apply_fixup_(wiki_df_val)\n",
    "\n",
    "if DEBUG:\n",
    "    print(f\"Trn: {len(wiki_trn_tok), len(wiki_trn_labels)}, Val: {len(wiki_val_tok), len(wiki_val_labels)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Now we make vocabulary, select 60k most freq words \\n        (we do this looking only at imdb, and ignore wiki here)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Now we make vocabulary, select 60k most freq words \n",
    "        (we do this looking only at imdb, and ignore wiki here)\n",
    "'''\n",
    "\n",
    "freq = Counter(p for o in trn_tok for p in o)\n",
    "# freq.most_common(25)\n",
    "max_vocab = 60000\n",
    "min_freq = 2\n",
    "\n",
    "itos = [o for o, c in freq.most_common(max_vocab) if c > min_freq]\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(0, '_unk_')\n",
    "stoi = collections.defaultdict(lambda: 0, {v: k for k, v in enumerate(itos)})\n",
    "vs = len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITOS: 9314, STOI: 25161\n",
      "ITOS: 9314, STOI: 36993\n"
     ]
    }
   ],
   "source": [
    "trn_lm = np.array([[stoi[o] for o in p] for p in trn_tok])\n",
    "val_lm = np.array([[stoi[o] for o in p] for p in val_tok])\n",
    "\n",
    "if DEBUG:\n",
    "    print(f\"ITOS: {len(itos)}, STOI: {len(stoi)}\")\n",
    "    \n",
    "wiki_trn_lm = np.array([[stoi[o] for o in p] for p in wiki_trn_tok])\n",
    "wiki_val_lm = np.array([[stoi[o] for o in p] for p in wiki_val_tok])\n",
    "    \n",
    "if DEBUG:\n",
    "    print(f\"ITOS: {len(itos)}, STOI: {len(stoi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Now we pull pretrained models from disk\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Now we pull pretrained models from disk\n",
    "\"\"\"\n",
    "em_sz, nh, nl = 400, 1150, 3\n",
    "# PRE_PATH = PATH / 'models' / 'wt103'\n",
    "# PRE_LM_PATH = PRE_PATH / 'fwd_wt103.h5'\n",
    "wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)\n",
    "enc_wgts = core.to_np(wgts['0.encoder.weight'])\n",
    "row_m = enc_wgts.mean(0)\n",
    "itos2 = pickle.load((PRE_PATH / 'itos_wt103.pkl').open('rb'))\n",
    "stoi2 = collections.defaultdict(lambda: -1, {v: k for k, v in enumerate(itos2)})\n",
    "new_w = np.zeros((vs, em_sz), dtype=np.float32)\n",
    "for i, w in enumerate(itos):\n",
    "    r = stoi2[w]\n",
    "    new_w[i] = enc_wgts[r] if r >= 0 else row_m\n",
    "\n",
    "wgts['0.encoder.weight'] = T(new_w)\n",
    "wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))\n",
    "wgts['1.decoder.weight'] = T(np.copy(new_w))\n",
    "wgts_enc = {'.'.join(k.split('.')[1:]): val\n",
    "            for k, val in wgts.items() if k[0] == '0'}\n",
    "wgts_dec = {'.'.join(k.split('.')[1:]): val\n",
    "            for k, val in wgts.items() if k[0] == '1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0,
     7,
     20
    ]
   },
   "outputs": [],
   "source": [
    "class CustomEncoder(lm_rnn.RNN_Encoder):\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return torch.nn.ModuleList([torch.nn.ModuleList([self.rnns[0], self.dropouths[0]]),\n",
    "                                    torch.nn.ModuleList([self.rnns[1], self.dropouths[1]]),\n",
    "                                    torch.nn.ModuleList([self.rnns[2], self.dropouths[2]])])\n",
    "class CustomLinear(text.LinearDecoder):\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return torch.nn.ModuleList([self.decoder, self.dropout])\n",
    "    \n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.dropout(outputs[-1])\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        result = decoded.view(-1, decoded.size(1))\n",
    "        return result, raw_outputs, outputs\n",
    "    \n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 _parameter_dict,\n",
    "                 _device,\n",
    "                 _wgts_e,\n",
    "                 _wgts_d,\n",
    "                 _encargs):\n",
    "        super(LanguageModel, self).__init__()\n",
    "\n",
    "        self.parameter_dict = _parameter_dict\n",
    "        self.device = _device\n",
    "\n",
    "        self.encoder = CustomEncoder(**_encargs).to(self.device)\n",
    "        self.encoder.load_state_dict(_wgts_e)\n",
    "        \"\"\"\n",
    "            Explanation:\n",
    "                400*3 because input is [ h_T, maxpool, meanpool ]\n",
    "                0.4, 0.1 are drops at various layersLM_PATH\n",
    "        \"\"\"\n",
    "        self.linear_dec = CustomLinear(\n",
    "            _encargs['ntoken'],\n",
    "            n_hid=400,\n",
    "            dropout=0.1 * 0.7,\n",
    "            tie_encoder=self.encoder.encoder,\n",
    "            bias=False\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.linear_dom = CustomLinear(\n",
    "            _encargs['ntoken'],\n",
    "            n_hid=2,\n",
    "            dropout=0.1 * 0.7,\n",
    "#             tie_encoder=self.encoder.encoder,\n",
    "            bias=False\n",
    "        ).to(self.device)\n",
    "        self.encoder.reset()\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Encoding all the data\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, x):\n",
    "        return self.linear_dec(x)[0]\n",
    "    \n",
    "    def domain(self, x):\n",
    "        return self.linear_dom(x)[0]\n",
    "    \n",
    "    @property\n",
    "    def layers(self):\n",
    "        layers = [x for x in self.encoder.layers]\n",
    "        layers += [x for x in self.linear.layers]\n",
    "        return torch.nn.ModuleList(layers)\n",
    "\n",
    "    @property\n",
    "    def layers_rev(self):\n",
    "        layers = [x for x in self.encoder.layers]\n",
    "        layers += [x for x in self.linear.layers]\n",
    "        layers.reverse()\n",
    "        return torch.nn.ModuleList(layers)\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            pred = self.forward(x)\n",
    "            self.train()\n",
    "            return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = 1e-7\n",
    "bptt = 70\n",
    "bs = 24\n",
    "opt_fn = partial(torch.optim.Adam, betas=(0.8, 0.99))  # @TODO: find real optimizer, and params\n",
    "\n",
    "# Load the pre-trained model\n",
    "parameter_dict = {'itos2': itos2}\n",
    "dps = list(np.asarray([0.25, 0.1, 0.2, 0.02, 0.15]) * 0.7)\n",
    "encargs = {'ntoken': new_w.shape[0],\n",
    "           'emb_sz': 400, 'n_hid': 1150,\n",
    "           'n_layers': 3, 'pad_token': 0,\n",
    "           'qrnn': False, 'dropouti': dps[0],\n",
    "           'wdrop': dps[2], 'dropoute': dps[3], 'dropouth': dps[4]}\n",
    "\n",
    "# For now, lets assume our best lr = 0.001\n",
    "bestlr = 0.001 * 10\n",
    "lm = LanguageModel(parameter_dict, device, wgts_enc, wgts_dec, encargs)\n",
    "opt = make_opt(lm, opt_fn, lr=bestlr)\n",
    "\n",
    "data_fn = partial(text.LanguageModelLoader, bs=bs, bptt=bptt)\n",
    "data_imdb = {'train': np.concatenate(trn_lm), 'valid': np.concatenate(val_lm)}\n",
    "data_wiki = {'train': np.concatenate(wiki_trn_lm), 'valid': np.concatenate(wiki_val_lm)}\n",
    "loss_main_fn = F.cross_entropy\n",
    "loss_aux_fn = nn.BCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = data_fn(data_imdb['train'])\n",
    "wiki = data_fn(data_wiki['train'])\n",
    "for dimdb in imdb:\n",
    "    break\n",
    "for dwiki in wiki:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones_like(dimdb[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1][-1].view(-1,  a[1][-1].shape[2]).shape\n",
    "lm.domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.domain(a)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = lm.encoder(dimdb[0])\n",
    "b = lm.decoder(a)[0]\n",
    "c = lm.domain(a)[0]\n",
    "loss_main = loss_main_fn(b, dimdb[1])\n",
    "loss_aux = loss_aux_fn(c, torch.ones_like(dimdb[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TempEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lin = nn.Linear(2, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lin "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
