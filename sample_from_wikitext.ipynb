{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# pretty print all cell's output and not just the last one\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Lib imports\n",
    "import collections\n",
    "import html\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ['QT_QPA_PLATFORM'] = 'offscreen'\n",
    "\n",
    "# FastAI Imports\n",
    "from fastai import text, core, lm_rnn\n",
    "\n",
    "# Torch imports\n",
    "import torch.nn as nn\n",
    "import torch.tensor as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Mytorch imports\n",
    "from mytorch import loops as mtlp\n",
    "from mytorch.utils.goodies import *\n",
    "from mytorch import lriters as mtlr\n",
    "\n",
    "import utils\n",
    "from options import Phase2 as params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "DEBUG = True\n",
    "TRIM = False\n",
    "\n",
    "# Path fields\n",
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "\n",
    "WIKI_DATA_PATH = Path('raw/wikitext/wikitext-103/')\n",
    "WIKI_DATA_PATH.mkdir(exist_ok=True)\n",
    "IMDB_DATA_PATH = Path('raw/imdb/aclImdb/')\n",
    "IMDB_DATA_PATH.mkdir(exist_ok=True)\n",
    "PATH = Path('resources/proc/imdb')\n",
    "DATA_PROC_PATH = PATH / 'data'\n",
    "DATA_LM_PATH = PATH / 'datalm'\n",
    "\n",
    "LM_PATH = Path('resources/models')\n",
    "LM_PATH.mkdir(exist_ok=True)\n",
    "PRE_PATH = LM_PATH / 'wt103'\n",
    "PRE_LM_PATH = PRE_PATH / 'fwd_wt103.h5'\n",
    "CLASSES = ['neg', 'pos', 'unsup']\n",
    "WIKI_CLASSES = ['wiki.train.tokens', 'wiki.valid.tokens', 'wiki.test.tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load **WIKI** data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_sent(x):\n",
    "    x = x.strip()\n",
    "    if len(x) == 0: return False\n",
    "    if x[0] == '=' and x[-1] == '=': return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def wiki_get_texts_org(path):\n",
    "    texts = []\n",
    "    for idx, label in enumerate(WIKI_CLASSES):\n",
    "        with open(path / label, encoding='utf-8') as f:\n",
    "            texts.append([sent.strip() for sent in f.readlines() if is_valid_sent(sent)])\n",
    "    return tuple(texts)\n",
    "\n",
    "\n",
    "wiki_trn_texts, wiki_val_texts, wiki_tst_texts = wiki_get_texts_org(WIKI_DATA_PATH)\n",
    "print(len(wiki_trn_texts), len(wiki_val_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle data & Make dummy labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(wiki_trn_texts)\n",
    "np.random.shuffle(wiki_val_texts)\n",
    "np.random.shuffle(wiki_tst_texts)\n",
    "\n",
    "wiki_trn_labels = [0 for _ in wiki_trn_texts]\n",
    "wiki_val_labels = [0 for _ in wiki_val_texts]\n",
    "wiki_tst_labels = [0 for _ in wiki_val_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe magic & Fixing up text & Splitting into train and Val\n",
    "\n",
    "_note_: We read generously from the valid data. Should do something about it!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     12,
     22,
     30
    ]
   },
   "outputs": [],
   "source": [
    "chunksize = 24000\n",
    "re1 = re.compile(r'  +')\n",
    "col_names = ['labels', 'text']\n",
    "\n",
    "def _fixup_(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
    "        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "\n",
    "def _get_texts_(df, n_lbls=1):\n",
    "    labels = df.iloc[:, range(n_lbls)].values.astype(np.int64)\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)\n",
    "    for i in range(n_lbls + 1, len(df.columns)): texts += f' {FLD} {i - n_lbls} ' + df[i].astype(str)\n",
    "    texts = list(texts.apply(_fixup_).values)\n",
    "\n",
    "    tok = text.Tokenizer().proc_all_mp(core.partition_by_cores(texts))\n",
    "    return tok, list(labels)\n",
    "\n",
    "\n",
    "def _simple_apply_fixup_(df):\n",
    "    labels = [0] * df.shape[0]\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df.text\n",
    "    texts = list(texts.apply(_fixup_).values)\n",
    "    tok = text.Tokenizer().proc_all_mp(core.partition_by_cores(texts))\n",
    "    return tok, list(labels)\n",
    "\n",
    "\n",
    "def get_all(df, n_lbls):\n",
    "    tok, labels = [], []\n",
    "    for i, r in enumerate(df):\n",
    "        print(i)\n",
    "        tok_, labels_ = _get_texts_(r, n_lbls)\n",
    "        tok += tok_;\n",
    "        labels += labels_\n",
    "    return tok, labels\n",
    "\n",
    "\n",
    "# trn_texts, val_texts = sklearn.model_selection.train_test_split(\n",
    "#         np.concatenate([trn_texts, val_texts]), test_size=0.1)\n",
    "\n",
    "# if DEBUG:\n",
    "#     print(len(trn_texts), len(val_texts))\n",
    "\n",
    "# # df_trn = pd.DataFrame({'text': trn_texts, 'labels': [0] * len(trn_texts)}, columns=col_names)\n",
    "# # df_val = pd.DataFrame({'text': val_texts, 'labels': [0] * len(val_texts)}, columns=col_names)\n",
    "\n",
    "# trn_tok, trn_labels = _simple_apply_fixup_(df_trn)\n",
    "# val_tok, val_labels = _simple_apply_fixup_(df_val)\n",
    "\n",
    "# if DEBUG:\n",
    "#     print(f\"Trn: {len(trn_tok), len(trn_labels)}, Val: {len(val_tok), len(val_labels)} \")\n",
    "\n",
    "wiki_trn_texts, wiki_val_texts = sklearn.model_selection.train_test_split(\n",
    "    np.concatenate([wiki_trn_texts, wiki_val_texts, wiki_tst_texts]), test_size=0.1)\n",
    "\n",
    "if DEBUG:\n",
    "    print(len(wiki_trn_texts), len(wiki_val_texts))\n",
    "\n",
    "wiki_df_trn = pd.DataFrame({'text': wiki_trn_texts, 'labels': [0] * len(wiki_trn_texts)}, columns=col_names)\n",
    "wiki_df_val = pd.DataFrame({'text': wiki_val_texts, 'labels': [0] * len(wiki_val_texts)}, columns=col_names)\n",
    "\n",
    "wiki_trn_tok, wiki_trn_labels = _simple_apply_fixup_(wiki_df_trn)\n",
    "wiki_val_tok, wiki_val_labels = _simple_apply_fixup_(wiki_df_val)\n",
    "\n",
    "if DEBUG:\n",
    "    print(f\"Trn: {len(wiki_trn_tok), len(wiki_trn_labels)}, Val: {len(wiki_val_tok), len(wiki_val_labels)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we make vocabulary, select 60k most freq words \n",
    " \n",
    " _note_: we do this looking only at imdb, and ignore wiki here **NOT ANY MORE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull itos from disk\n",
    "itos = pickle.load((PRE_PATH / 'itos_wt103.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda: 0, {v: k for k, v in enumerate(itos)})\n",
    "vs = len(itos)\n",
    "# trn_lm = np.array([[stoi[o] for o in p] for p in trn_tok])\n",
    "# val_lm = np.array([[stoi[o] for o in p] for p in val_tok])\n",
    "\n",
    "if DEBUG:\n",
    "    print(f\"ITOS: {len(itos)}, STOI: {len(stoi)}\")\n",
    "\n",
    "wiki_trn_lm = np.array([[stoi[o] for o in p] for p in wiki_trn_tok])\n",
    "wiki_val_lm = np.array([[stoi[o] for o in p] for p in wiki_val_tok])\n",
    "\n",
    "if DEBUG:\n",
    "    print(f\"ITOS: {len(itos)}, STOI: {len(stoi)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we pull pretrained models from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz, nh, nl = 400, 1150, 3\n",
    "# PRE_PATH = PATH / 'models' / 'wt103'\n",
    "# PRE_LM_PATH = PRE_PATH / 'fwd_wt103.h5'\n",
    "wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)\n",
    "enc_wgts = core.to_np(wgts['0.encoder.weight'])\n",
    "\n",
    "wgts_enc = {'.'.join(k.split('.')[1:]): val\n",
    "            for k, val in wgts.items() if k[0] == '0'}\n",
    "wgts_dec = {'.'.join(k.split('.')[1:]): val\n",
    "            for k, val in wgts.items() if k[0] == '1'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts_enc['encoder.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     9,
     23,
     41
    ]
   },
   "outputs": [],
   "source": [
    "class CustomEncoder(lm_rnn.RNN_Encoder):\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return torch.nn.ModuleList([torch.nn.ModuleList([self.rnns[0], self.dropouths[0]]),\n",
    "                                    torch.nn.ModuleList([self.rnns[1], self.dropouths[1]]),\n",
    "                                    torch.nn.ModuleList([self.rnns[2], self.dropouths[2]])])\n",
    "\n",
    "\n",
    "class CustomDecoder(text.LinearDecoder):\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return torch.nn.ModuleList([self.decoder, self.dropout])\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.dropout(outputs[-1])\n",
    "        decoded = self.decoder(output.view(output.size(0) * output.size(1), output.size(2)))\n",
    "        result = decoded.view(-1, decoded.size(1))\n",
    "        return result, raw_outputs, outputs\n",
    "\n",
    "\n",
    "class CustomLinear(lm_rnn.PoolingLinearClassifier):\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = outputs[-1]\n",
    "        sl,bs,_ = output.size()\n",
    "        avgpool = self.pool(output, bs, False)\n",
    "        mxpool = self.pool(output, bs, True)\n",
    "        x = torch.cat([output[-1], mxpool, avgpool], 1)\n",
    "        for i, l in enumerate(self.layers):\n",
    "            l_x = l(x)\n",
    "            if i != len(self.layers) -1:\n",
    "                x = F.relu(l_x)\n",
    "            else:\n",
    "                x = torch.sigmoid(l_x)\n",
    "        return l_x, raw_outputs, outputs\n",
    "\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 _parameter_dict,\n",
    "                 _device,\n",
    "                 _wgts_e,\n",
    "                 _wgts_d,\n",
    "                 _encargs):\n",
    "        super(LanguageModel, self).__init__()\n",
    "\n",
    "        self.parameter_dict = _parameter_dict\n",
    "        self.device = _device\n",
    "\n",
    "        self.encoder = CustomEncoder(**_encargs).to(self.device)\n",
    "        self.encoder.load_state_dict(_wgts_e)\n",
    "        \"\"\"\n",
    "            Explanation:\n",
    "                400*3 because input is [ h_T, maxpool, meanpool ]\n",
    "                0.4, 0.1 are drops at various layersLM_PATH\n",
    "        \"\"\"\n",
    "        self.linear_dec = CustomDecoder(\n",
    "            _encargs['ntoken'],\n",
    "            n_hid=400,\n",
    "            dropout=params.decoder_drops,\n",
    "            tie_encoder=self.encoder.encoder,\n",
    "            bias=False\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.linear_dec.load_state_dict(_wgts_d)\n",
    "        self.encoder.reset()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_enc = self.encoder(x)\n",
    "        return self.linear_dec(x_enc)\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        # layers = [x for x in self.encoder.layers]\n",
    "        # layers.append(torch.nn.ModuleList([x for x in self.linear_dec.layers]))\n",
    "        # layers.append(torch.nn.ModuleList([x for x in self.linear_dom.layers]))\n",
    "        # return torch.nn.ModuleList(layers)\n",
    "        return self.encoder.layers.extend(self.linear_dec.layers)\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            pred = self.forward(x)\n",
    "            self.train()\n",
    "            return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 70\n",
    "bs = params.bs\n",
    "opt_fn = partial(torch.optim.SGD)  #, betas=params.adam_betas)  # @TODO: find real optimizer, and params\n",
    "\n",
    "# Load the pre-trained model\n",
    "parameter_dict = {'itos': itos}\n",
    "dps = params.encoder_drops\n",
    "encargs = {'ntoken': wgts_enc['encoder.weight'].shape[0],\n",
    "           'emb_sz': 400, 'n_hid': 1150,\n",
    "           'n_layers': 3, 'pad_token': 0,\n",
    "           'qrnn': False, 'dropouti': dps[0],\n",
    "           'wdrop': dps[2], 'dropoute': dps[3], 'dropouth': dps[4]}\n",
    "\n",
    "lm = LanguageModel(parameter_dict, device, _wgts_e=wgts_enc, _wgts_d=wgts_dec, _encargs=encargs)\n",
    "opt = make_opt(lm, opt_fn, lr=params.lr.init)\n",
    "loss_main_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make data iterators and LR iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(seq: list, model: LanguageModel):\n",
    "    _seq = torch.tensor(seq, dtype=torch.long, device=device).unsqueeze(-1)\n",
    "    return torch.argmax(model.predict(_seq)[0], dim=-1).detach().cpu().numpy()\n",
    "def tostr(seq, itos):\n",
    "    return ' '.join([itos[x] for x in seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ypred = get_predictions(wiki_trn_lm[10], lm)\n",
    "_x, _ypred = tostr(wiki_trn_lm[10], itos), tostr(ypred, itos) \n",
    "_x, _ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalize the above for the entire thing\n",
    "y = []\n",
    "for sent in tqdm(wiki_trn_lm):\n",
    "    y.append(get_predictions(sent, lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temp Holdout\n",
    "due to processing time. Will pickle and resume again, maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'x':wiki_trn_lm, 'y':y}\n",
    "loc = './resources/proc/imdb/enforce_phase1/temp.pkl'\n",
    "pickle.dump(data, open(loc, 'wb+'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back in Business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = []\n",
    "for sent in tqdm(wiki_val_lm):\n",
    "    y_val.append(get_predictions(sent, lm))\n",
    "data = {'x':wiki_val_lm, 'y':y_val}\n",
    "loc = './resources/proc/imdb/enforce_phase1/temp_val.pkl'\n",
    "pickle.dump(data, open(loc, 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
