{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T14:42:10.742089Z",
     "start_time": "2019-04-05T14:42:10.736908Z"
    },
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# pretty print all cell's output and not just the last one\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T14:42:12.655602Z",
     "start_time": "2019-04-05T14:42:10.749469Z"
    },
    "code_folding": [
     0
    ],
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from functools import partial\n",
    "from typing import Tuple, Union, Callable\n",
    "\n",
    "# Torch imports\n",
    "import torch.nn as nn\n",
    "import torch.tensor as tensor\n",
    "import torch.nn.functional as func\n",
    "\n",
    "# Mytorch imports\n",
    "from mytorch import loops as mtlp\n",
    "from mytorch.utils.goodies import *\n",
    "from mytorch import lriters as mtlr\n",
    "\n",
    "# Local imports\n",
    "from utils import dann_loop\n",
    "from data import DataPuller\n",
    "from options import Phase2 as params\n",
    "\n",
    "os.environ['QT_QPA_PLATFORM'] = 'offscreen'\n",
    "\n",
    "# FastAI Imports\n",
    "from fastai import text, core, lm_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T14:42:12.660920Z",
     "start_time": "2019-04-05T14:42:12.657717Z"
    },
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "QUICK = True\n",
    "DEBUG = True\n",
    "PRETRAINED = True\n",
    "MESSAGE = \"A new start\"\n",
    "SAFE_MODE = True\n",
    "DATASETS = \"imdb\".split(',')\n",
    "\n",
    "if len(DATASETS) < 2: params.loss_scale = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T14:42:12.714781Z",
     "start_time": "2019-04-05T14:42:12.662761Z"
    },
    "code_folding": [
     25,
     94,
     101,
     107,
     122,
     141,
     196,
     210
    ],
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "DEVICE = 'cuda'\n",
    "KNOWN_DATASETS = ['imdb', 'wikitext', '']\n",
    "\n",
    "device = torch.device(DEVICE)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Path fields\n",
    "PATH = Path('resources/proc/imdb')\n",
    "DATA_PROC_PATH = PATH / 'data'\n",
    "DATA_LM_PATH = PATH / 'datalm'\n",
    "DUMP_PATH = Path('resources/models/runs')\n",
    "\n",
    "LM_PATH = Path('resources/models')\n",
    "LM_PATH.mkdir(exist_ok=True)\n",
    "PRE_PATH = LM_PATH / 'wt103'\n",
    "PRE_LM_PATH = PRE_PATH / 'fwd_wt103.h5'\n",
    "\n",
    "'''\n",
    "    Data sampler for this training\n",
    "'''\n",
    "\n",
    "\n",
    "# noinspection PyShadowingNames\n",
    "class DomainAgnosticSampler:\n",
    "    \"\"\" Sample data for language model training from two different domains in one batch. \"\"\"\n",
    "\n",
    "    def __init__(self, data: Tuple[Union[list, np.ndarray], Union[list, np.ndarray]], data_fn: Callable):\n",
    "        \"\"\"\n",
    "            Here, data_fn would be something like\n",
    "                `partial(text.LanguageModelLoader, bs=bs, bptt=bptt)`\n",
    "            And data_a/b would be something like\n",
    "                `{'train': np.concatenate(trn_lm), 'valid': np.concatenate(val_lm)}['train']`\n",
    "            data_fn (fastai's language model loader) flattens y and returns x of seqlen, batchsize\n",
    "        \"\"\"\n",
    "        self.args = {'data_fn': data_fn, 'data': data,}\n",
    "        self.iters = [iter([]) for _ in range(len(data))]\n",
    "        self.reset(**self.args)\n",
    "\n",
    "    def reset(self, data_fn: Callable, data: list):\n",
    "        self.iters = [iter(data_fn(data_)) for data_ in data]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        x,y = [], []\n",
    "        for iter_ in self.iters:\n",
    "            x_, y_ = iter_.__next__()\n",
    "            x.append(x_)\n",
    "            y.append(y_)\n",
    "        return self._combine_batch_(x, y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min([len(self.args['data_fn'](data_)) for data_ in self.args['data'] ])\n",
    "\n",
    "    @staticmethod\n",
    "    def _combine_batch_(x, y):\n",
    "        \"\"\"\n",
    "            :param x is a list of np.arr looks like seqlen, batchsize\n",
    "            :param y is a corresponding list of np.arr (one word ahead than x_a) which is a flattened x_a.shape mat\n",
    "\n",
    "             Returns x, y, y_dom in similar shapes as input\n",
    "        \"\"\"\n",
    "\n",
    "        # Get them to interpretable shapes\n",
    "        y = [y_.reshape(x[i].shape).transpose(1, 0) for i,y_ in enumerate(y)]\n",
    "        x = [x_.transpose(1, 0) for x_ in x]\n",
    "\n",
    "        b_bs, b_sl = x[0].shape[0], min([x_.shape[1] for x_ in x])\n",
    "\n",
    "        # Concatenate to make an x and y\n",
    "        x = np.concatenate([x_[:, :b_sl] for x_ in x])\n",
    "        y = np.concatenate([y_[:, :b_sl] for y_ in y])\n",
    "\n",
    "        # Shuffle and remember shuffle index to make y labels for domain agnostic training\n",
    "        intrp = np.arange(b_bs * 2)\n",
    "        np.random.shuffle(intrp)\n",
    "        y_dom = (intrp >= b_bs) * 1\n",
    "        x = x[intrp]\n",
    "        y = y[intrp]\n",
    "\n",
    "        x = x.transpose(1, 0)\n",
    "        y = y.transpose(1, 0).reshape(np.prod(y.shape))\n",
    "\n",
    "        return x, y, y_dom\n",
    "\n",
    "\n",
    "'''\n",
    "    Model definitions\n",
    "'''\n",
    "\n",
    "\n",
    "class CustomEncoder(lm_rnn.RNN_Encoder):\n",
    "\n",
    "    def forward(self, input, domain=None):\n",
    "        \"\"\" Overwrote fn to keep the interface same b/w phase 2 & phase 3 models (same training loop)\"\"\"\n",
    "        return super().forward(input)\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return torch.nn.ModuleList([torch.nn.ModuleList([self.rnns[0], self.dropouths[0]]),\n",
    "                                    torch.nn.ModuleList([self.rnns[1], self.dropouths[1]]),\n",
    "                                    torch.nn.ModuleList([self.rnns[2], self.dropouths[2]])])\n",
    "\n",
    "\n",
    "class CustomDecoder(text.LinearDecoder):\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return torch.nn.ModuleList([self.decoder, self.dropout])\n",
    "\n",
    "    def forward(self, x):\n",
    "        raw_outputs, outputs = x\n",
    "        output = self.dropout(outputs[-1])\n",
    "        decoded = self.decoder(output.view(output.size(0) * output.size(1), output.size(2)))\n",
    "        result = decoded.view(-1, decoded.size(1))\n",
    "        return result, (raw_outputs, outputs)\n",
    "\n",
    "\n",
    "# noinspection PyShadowingNames\n",
    "class CustomLinear(lm_rnn.PoolingLinearClassifier):\n",
    "\n",
    "    def forward(self, x):\n",
    "        raw_outputs, outputs = x\n",
    "        output = outputs[-1]\n",
    "        sl,bs,_ = output.size()\n",
    "        avgpool = self.pool(output, bs, False)\n",
    "        mxpool = self.pool(output, bs, True)\n",
    "        x = torch.cat([output[-1], mxpool, avgpool], 1)\n",
    "        for i, l in enumerate(self.layers):\n",
    "            l_x = l(x)\n",
    "            if i != len(self.layers) - 1:\n",
    "                x = func.relu(l_x)\n",
    "            else:\n",
    "                x = torch.sigmoid(l_x)\n",
    "        # noinspection PyUnboundLocalVariable\n",
    "        return l_x, (raw_outputs, outputs)\n",
    "\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 _parameter_dict,\n",
    "                 _device,\n",
    "                 _encargs,\n",
    "                 _n_tasks=2,\n",
    "                 _wgts_e=None,\n",
    "                 _wgts_d=None):\n",
    "        super(LanguageModel, self).__init__()\n",
    "\n",
    "        self.parameter_dict = _parameter_dict\n",
    "        self.device = _device\n",
    "\n",
    "        self.encoder = CustomEncoder(**_encargs).to(self.device)\n",
    "        if _wgts_e:\n",
    "            self.encoder.load_state_dict(_wgts_e)\n",
    "        \"\"\"\n",
    "            Explanation:\n",
    "                400*3 because input is [ h_T, maxpool, meanpool ]\n",
    "                0.4, 0.1 are drops at various layersLM_PATH\n",
    "        \"\"\"\n",
    "        self.linear_dec = CustomDecoder(\n",
    "            _encargs['ntoken'],\n",
    "            n_hid=400,\n",
    "            dropout=params.decoder_drops,\n",
    "            tie_encoder=self.encoder.encoder,\n",
    "            bias=False\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.linear_dom = CustomLinear(layers=params.domclas_layers + [_n_tasks], drops=params.domclas_drops).to(self.device)\n",
    "        self.encoder.reset()\n",
    "\n",
    "    def forward(self, x, d):\n",
    "        \"\"\" d is not used (only so the loop remains same b/w phase 2 and phase 3 models) \"\"\"\n",
    "        x_enc = self.encoder(x, d)\n",
    "        return self.linear_dec(x_enc)\n",
    "\n",
    "    def domain(self, x_enc):\n",
    "        x_enc = list(x_enc)\n",
    "        x_enc[1] = [GradReverse.apply(enc_tensr) for enc_tensr in x_enc[1]]\n",
    "        return self.linear_dom(x_enc)[0]\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return self.encoder.layers.extend(self.linear_dec.layers).extend(self.linear_dom.layers)\n",
    "\n",
    "    def predict(self, x, d):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            pred = self.forward(x, d)\n",
    "            self.train()\n",
    "            return pred\n",
    "\n",
    "\n",
    "def _eval(y_pred, y_true, tasks: int=1, task_index: torch.tensor=None):\n",
    "    \"\"\"\n",
    "        Expects a batch of input\n",
    "\n",
    "        :param y_pred: tensor of shape (b, nc)\n",
    "        :param y_true: tensor of shape (b, 1)\n",
    "    \"\"\"\n",
    "    return torch.mean((torch.argmax(y_pred, dim=1) == y_true).float())\n",
    "\n",
    "\n",
    "def loss_wrapper(y_pred, y_true, loss_fn, **args):\n",
    "    return loss_fn(y_pred, y_true)\n",
    "\n",
    "\n",
    "class CustomLanguageModelLoader(text.LanguageModelLoader):\n",
    "    \"\"\" Overwriting the class so we can call it within the same way of iterating over data as in other cases.\"\"\"\n",
    "\n",
    "    def __init__(self, data, **args):\n",
    "        super().__init__(data, **args)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.i, self.iter = 0, 0\n",
    "        while self.i < self.n - 1 and self.iter < len(self):\n",
    "            if self.i == 0:\n",
    "                seq_len = self.bptt + 5 * 5\n",
    "            else:\n",
    "                bptt = self.bptt if np.random.random() < 0.95 else self.bptt / 2.\n",
    "                seq_len = max(5, int(np.random.normal(bptt, 5)))\n",
    "            res = self.get_batch(self.i, seq_len)\n",
    "            _res = list(res) + [torch.zeros(res[0].shape[1])]\n",
    "            self.i += seq_len\n",
    "            self.iter += 1\n",
    "            yield _res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T14:42:12.935402Z",
     "start_time": "2019-04-05T14:42:12.716594Z"
    },
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# Check args.\n",
    "if DATASETS == ['']:\n",
    "    DATASETS = []\n",
    "else:\n",
    "    for dataset in DATASETS:\n",
    "        assert dataset in KNOWN_DATASETS, f\"Couldn't find a dataset called {dataset}. Exiting.\"\n",
    "\n",
    "params.message = MESSAGE\n",
    "params.quick = QUICK\n",
    "params.datasets = DATASETS\n",
    "\n",
    "if DEBUG:\n",
    "    print(\"Pulling data from disk\")\n",
    "\n",
    "# Pulling data from disk\n",
    "data_puller = DataPuller(debug=False, max_vocab=params.max_vocab_task, min_freq=params.min_vocab_freq, trim_trn=1000, trim_val=1000)\n",
    "\n",
    "trn_lm, val_lm = [], []\n",
    "for dataset in DATASETS:\n",
    "\n",
    "    trn_lm_, val_lm_, itos = data_puller.get(dataset, supervised=False, trim=params.quick, cached=True, merge_vocab=params.max_vocab_others)\n",
    "\n",
    "    # Append data to main lists\n",
    "    trn_lm.append(trn_lm_)\n",
    "    val_lm.append(val_lm_)\n",
    "\n",
    "vs = len(itos) if len(DATASETS) > 0 else 10\n",
    "\n",
    "\"\"\"\n",
    "    Now we pull pretrained models from disk    \n",
    "\"\"\"\n",
    "\n",
    "if DEBUG:\n",
    "    print(\"Pulling models from disk\")\n",
    "\n",
    "em_sz, nh, nl = 400, 1150, 3\n",
    "wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)\n",
    "enc_wgts = core.to_np(wgts['0.encoder.weight'])\n",
    "row_m = enc_wgts.mean(0)\n",
    "itos2 = pickle.load((PRE_PATH / 'itos_wt103.pkl').open('rb'))\n",
    "stoi2 = collections.defaultdict(lambda: -1, {v: k for k, v in enumerate(itos2)})\n",
    "new_w = np.zeros((vs, em_sz), dtype=np.float32)\n",
    "for i, w in enumerate(itos):\n",
    "    r = stoi2[w]\n",
    "    new_w[i] = enc_wgts[r] if r >= 0 else row_m\n",
    "\n",
    "# noinspection PyCallingNonCallable\n",
    "wgts['0.encoder.weight'] = tensor(new_w)\n",
    "# noinspection PyCallingNonCallable\n",
    "wgts['0.encoder_with_dropout.embed.weight'] = tensor(np.copy(new_w))\n",
    "# noinspection PyCallingNonCallable\n",
    "wgts['1.decoder.weight'] = tensor(np.copy(new_w))\n",
    "wgts_enc = {'.'.join(k.split('.')[1:]): val\n",
    "            for k, val in wgts.items() if k[0] == '0'}\n",
    "wgts_dec = {'.'.join(k.split('.')[1:]): val\n",
    "            for k, val in wgts.items() if k[0] == '1'}\n",
    "\n",
    "'''\n",
    "    Setting up things for training.\n",
    "'''\n",
    "bptt = 70\n",
    "bs = params.bs\n",
    "opt_fn = partial(torch.optim.SGD)  # , betas=params.adam_betas)\n",
    "lengths = np.array([len(CustomLanguageModelLoader(np.concatenate(trn_lm_), bs=bs, bptt=bptt)) for trn_lm_ in trn_lm])\n",
    "# l_a, l_b = len(text.LanguageModelLoader(np.concatenate(trn_lm), bs=bs, bptt=bptt)), \\\n",
    "#            len(text.LanguageModelLoader(np.concatenate(wiki_trn_lm), bs=bs, bptt=bptt))\n",
    "weights = torch.tensor(np.ascontiguousarray((lengths/np.sum(lengths))[::-1]), dtype=torch.float, device=device) if len(DATASETS) > 1 else None\n",
    "\n",
    "# Load the pre-trained model\n",
    "parameter_dict = {'itos2': itos2}\n",
    "dps = params.encoder_drops\n",
    "encargs = {'ntoken': new_w.shape[0],\n",
    "           'emb_sz': 400, 'n_hid': 1150,\n",
    "           'n_layers': 3, 'pad_token': 0,\n",
    "           'qrnn': False, 'dropouti': dps[0],\n",
    "           'wdrop': dps[2], 'dropoute': dps[3], 'dropouth': dps[4]}\n",
    "\n",
    "lm = LanguageModel(parameter_dict, device, _encargs=encargs, _n_tasks=len(DATASETS),\n",
    "                   _wgts_e=wgts_enc if PRETRAINED else None, _wgts_d=wgts_dec)\n",
    "opt = make_opt(lm, opt_fn, lr=params.lr.init)\n",
    "loss_main_fn = partial(loss_wrapper, loss_fn=func.cross_entropy)\n",
    "loss_aux_fn = partial(loss_wrapper, loss_fn=nn.CrossEntropyLoss(weights))\n",
    "\n",
    "# Make data\n",
    "if len(DATASETS) > 1:\n",
    "    data_fn_unidomain = partial(text.LanguageModelLoader, bs=bs, bptt=bptt)\n",
    "    data_train = [np.concatenate(trn_lm_) for trn_lm_ in trn_lm]\n",
    "    data_valid = [np.concatenate(val_lm_) for val_lm_ in val_lm]\n",
    "    data = {'train': data_train, 'valid': data_valid}\n",
    "    data_fn = partial(DomainAgnosticSampler, data_fn=data_fn_unidomain)\n",
    "elif len(DATASETS) == 1:\n",
    "    data_fn_unidomain = partial(CustomLanguageModelLoader, bs=bs, bptt=bptt)\n",
    "    data_train = [np.concatenate(trn_lm_) for trn_lm_ in trn_lm][0]\n",
    "    data_valid = [np.concatenate(val_lm_) for val_lm_ in val_lm][0]\n",
    "    data = {'train': data_train, 'valid': data_valid}\n",
    "    data_fn = data_fn_unidomain\n",
    "else:\n",
    "    # No dataset\n",
    "    data_train, data_valid, data, data_fn = [None] * 4\n",
    "\n",
    "# Set up lr and freeze stuff\n",
    "for grp in opt.param_groups:\n",
    "    grp['lr'] = 0.0\n",
    "opt.param_groups[3]['lr'] = params.lr.init\n",
    "opt.param_groups[4]['lr'] = params.lr.init\n",
    "\n",
    "# lr_args = {'batches':, 'cycles': 1}\n",
    "if len(DATASETS) > 0:\n",
    "    lr_args = {'iterations': len(data_fn(data=data['train'])),\n",
    "               'cut_frac': params.lr.sltr_cutfrac, 'ratio': params.lr.sltr_ratio}\n",
    "    lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.SlantedTriangularLR)\n",
    "    \n",
    "# Find places to save model\n",
    "save_dir = mt_save_dir(DUMP_PATH / '_'.join(DATASETS), _newdir=True) if not SAFE_MODE else ''\n",
    "save_fnames = {'torch_stuff':\n",
    "                   {'hightrn':\n",
    "                        {'model': 'unsup_model_hightrn.torch',\n",
    "                         'enc': 'unsup_model_hightrn_enc.torch'},\n",
    "                    'lowaux':\n",
    "                        {'model': 'unsup_model_lowaux.torch',\n",
    "                         'enc': 'unsup_model_lowaux_enc.torch'}}}\n",
    "\n",
    "\n",
    "\n",
    "if not SAFE_MODE:\n",
    "    # Dump the model and vocab as it is!\n",
    "    mt_save(save_dir, message=MESSAGE,\n",
    "            torch_stuff=[tosave('unsup_model_final.torch', lm.state_dict()),\n",
    "                         tosave('unsup_model_enc_final.torch', lm.encoder.state_dict())],\n",
    "            pickle_stuff=[tosave('itos.pkl', itos), tosave('unsup_options.pkl', params)])\n",
    "\n",
    "# Nothing more to do. Quit.\n",
    "warnings.warn(\"No dataset specified. Dumped the model, and vocab. Quitting the code now. Tschuss!\")\n",
    "# exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
