{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# pretty print all cell's output and not just the last one\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Lib imports\n",
    "import os\n",
    "from typing import List\n",
    "from functools import partial\n",
    "\n",
    "os.environ['QT_QPA_PLATFORM'] = 'offscreen'\n",
    "\n",
    "# FastAI Imports\n",
    "from fastai import text, lm_rnn\n",
    "\n",
    "# Torch imports\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Mytorch imports\n",
    "from mytorch.utils.goodies import *\n",
    "from mytorch import loops, lriters as mtlr\n",
    "\n",
    "# Local imports\n",
    "import main as p2\n",
    "import utils\n",
    "from data import DataPuller\n",
    "from options import Phase3 as params, Phase2 as p2params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUICK = False\n",
    "DEBUG = True\n",
    "MODEL_NUM = 3\n",
    "PRETRAINED = True\n",
    "MODEL_SUFFIX = ''\n",
    "SAFE_MODE = False\n",
    "PATH = Path('resources/proc/imdb')\n",
    "UNSUP_MODEL_DIR = PATH / 'models' / MODEL_NUM\n",
    "DATASETS = 'imdb,trec'.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0.0,
     58.0,
     66.0
    ]
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "'''\n",
    "    Paths and macros\n",
    "'''\n",
    "DATA_PROC_PATH = PATH / 'data'\n",
    "DATA_LM_PATH = PATH / 'datalm'\n",
    "\n",
    "LM_PATH = Path('resources/models')\n",
    "LM_PATH.mkdir(exist_ok=True)\n",
    "PRE_PATH = LM_PATH / 'wt103'\n",
    "PRE_LM_PATH = PRE_PATH / 'fwd_wt103.h5'\n",
    "KNOWN_DATASETS = {'imdb': 2, 'trec': 6}\n",
    "\n",
    "\n",
    "'''\n",
    "    Models, Data Samplers etc\n",
    "'''\n",
    "\n",
    "\n",
    "class CustomEncoder(lm_rnn.MultiBatchRNN):\n",
    "    @property\n",
    "    def layers(self):\n",
    "        # TODO: ADD ENCODERR!!!!!!!!!!\n",
    "        return torch.nn.ModuleList([torch.nn.ModuleList([self.rnns[0], self.dropouths[0]]),\n",
    "                                    torch.nn.ModuleList([self.rnns[1], self.dropouths[1]]),\n",
    "                                    torch.nn.ModuleList([self.rnns[2], self.dropouths[2]])])\n",
    "\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "\n",
    "    # @TODO: inject comments.\n",
    "    def __init__(self,\n",
    "                 _device: torch.device,\n",
    "                 n_token: int,\n",
    "                 dps: list,\n",
    "                 n_classes: List[int],\n",
    "                 enc_wgts = None,\n",
    "                 _debug=False):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        \"\"\"\n",
    "        :param n_token: int representing vocab size\n",
    "        :param n_classes: list representing multiple classes, each by its number of classes.\n",
    "            eg. n_classes = [2] -> one task; with 2 classes\n",
    "            eg. n_classes = [2, 6] -> two tasks, first with 2 classes, and one with 6.\n",
    "        \"\"\"\n",
    "\n",
    "        self.device = _device\n",
    "\n",
    "        # Load the pre-trained model\n",
    "        encargs = {'ntoken': n_token, 'emb_sz': 400, 'n_hid': 1150,\n",
    "                   'n_layers': 3, 'pad_token': 0, 'qrnn': False, 'bptt': 70, 'max_seq': 1400,\n",
    "                   'dropouti': dps[0], 'wdrop': dps[1], 'dropoute': dps[2], 'dropouth': dps[3]}\n",
    "        self.encoder = CustomEncoder(**encargs).to(self.device)\n",
    "\n",
    "        if enc_wgts:\n",
    "            self.encoder.load_state_dict(enc_wgts)\n",
    "\n",
    "        '''\n",
    "            Make multiple classifiers (depending upon n_classes)\n",
    "            \n",
    "            \n",
    "            Explanation:\n",
    "                400*3 because input is [ h_T, maxpool, meanpool ]\n",
    "                50 is hidden layer dim\n",
    "                2 is n_classes\n",
    "\n",
    "                0.4, 0.1 are drops at various layers\n",
    "        '''\n",
    "        self.linear = [text.PoolingLinearClassifier(layers=[400 * 3, 50, cls], drops=[dps[4], 0.1]).to(self.device)\n",
    "                       for cls in n_classes]\n",
    "        self.domain_clf = p2.CustomLinear(layers=p2params.domclas_layers, drops=p2params.domclas_drops).to(self.device)\n",
    "        self.encoder.reset()\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        layers = [x for x in self.encoder.layers]\n",
    "        layers += [x for linear in self.linear for x in linear.layers]\n",
    "        layers += [x for x in self.domain_clf.layers]\n",
    "        return torch.nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x: torch.tensor, domain: torch.tensor):\n",
    "        \"\"\" x is sl*bs; dom is bs indicating the task. \"\"\"\n",
    "\n",
    "        # Encoding all the data\n",
    "        x_proc = self.encoder(x.transpose(1, 0))\n",
    "\n",
    "        score = []\n",
    "        for dom in domain:\n",
    "            score.append(self.linear[dom.item()](x_proc)[0])\n",
    "\n",
    "        score = torch.cat(score)\n",
    "\n",
    "        return score, x_proc\n",
    "\n",
    "    def domain(self, x_proc):\n",
    "        x_proc = list(x_proc)\n",
    "        x_proc[1] = [GradReverse.apply(enc_tensr) for enc_tensr in x_proc[1]]\n",
    "        return self.domain_clf(x_proc)[0]\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            predicted, _ = self.forward(x)\n",
    "            self.train()\n",
    "            return predicted\n",
    "\n",
    "\n",
    "def epoch_end_hook() -> None:\n",
    "    lr_schedule.reset()\n",
    "\n",
    "\n",
    "# noinspection PyUnresolvedReferences\n",
    "def _eval(y_pred, y_true):\n",
    "    \"\"\"\n",
    "        Expects a batch of input\n",
    "\n",
    "        :param y_pred: tensor of shape (b, nc)\n",
    "        :param y_true: tensor of shape (b, 1)\n",
    "    \"\"\"\n",
    "    return torch.mean((torch.argmax(y_pred, dim=1) == y_true).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert MODEL_SUFFIX in ['_lowaux', '_hightrn', '', '_final'], 'Incorrect Suffix given with which to load model'\n",
    "\n",
    "params.quick = QUICK\n",
    "params.model_dir = str(UNSUP_MODEL_DIR) + ' and ' + str(MODEL_NUM)\n",
    "params.model_suffix = MODEL_SUFFIX\n",
    "params.datasets = DATASETS\n",
    "\n",
    "# Create representations of text using old itos\n",
    "itos_path = UNSUP_MODEL_DIR / 'itos.pkl'\n",
    "itos2 = pickle.load(itos_path.open('rb'))\n",
    "stoi2 = {v: k for k, v in enumerate(itos2)}\n",
    "\n",
    "data_puller = DataPuller(debug=False, max_vocab=params.max_vocab_task, min_freq=params.min_vocab_freq, trim_trn=1000, trim_val=-1)\n",
    "\n",
    "trn_texts_a, trn_labels_a, val_texts_a, val_labels_a, _ = data_puller.get(DATASETS[0], supervised=True,\n",
    "                                                                          trim=params.quick, cached=not params.quick)\n",
    "# Lose label 2 from imdb\n",
    "if DATASETS[0] == 'imdb':\n",
    "    trn_texts_a = trn_texts_a[trn_labels_a < 2]\n",
    "    trn_labels_a = trn_labels_a[trn_labels_a < 2]\n",
    "\n",
    "trn_texts_a = np.array([[stoi2.get(w, 0) for w in para] for para in trn_texts_a])\n",
    "val_texts_a = np.array([[stoi2.get(w, 0) for w in para] for para in val_texts_a])\n",
    "\n",
    "if len(DATASETS) > 1:\n",
    "    trn_texts_b, trn_labels_b, val_texts_b, val_labels_b, itos = data_puller.get(DATASETS[1], supervised=True,\n",
    "                                                                                 trim=params.quick, merge_vocab=params.max_vocab_wiki)\n",
    "\n",
    "    if DATASETS[1] == 'imdb':\n",
    "        trn_texts_b = trn_texts_b[trn_labels_b < 2]\n",
    "        trn_labels_b = trn_labels_b[trn_labels_b < 2]\n",
    "\n",
    "    trn_texts_b = np.array([[stoi2.get(w, 0) for w in para] for para in trn_texts_b])\n",
    "    val_texts_b = np.array([[stoi2.get(w, 0) for w in para] for para in val_texts_b])\n",
    "\n",
    "'''\n",
    "    Make model\n",
    "'''\n",
    "dps = list(params.encoder_dropouts)\n",
    "# enc_wgts = torch.load(LM_PATH, map_location=lambda storage, loc: storage)\n",
    "enc_wgts = torch.load(UNSUP_MODEL_DIR / ('unsup_model_enc' + MODEL_SUFFIX + '.torch'), map_location=lambda storage, loc: storage)\n",
    "n_classes = [KNOWN_DATASETS[d] for d in DATASETS]\n",
    "clf = TextClassifier(device, len(itos2), dps, enc_wgts=enc_wgts if PRETRAINED else None, n_classes=n_classes)\n",
    "\n",
    "'''\n",
    "    Setup things for training (data, loss, opt, lr schedule etc\n",
    "'''\n",
    "bs = params.bs\n",
    "loss_main_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_aux_fn = torch.nn.CrossEntropyLoss()\n",
    "opt_fn = partial(optim.Adam, betas=params.adam_betas)\n",
    "opt = make_opt(clf, opt_fn, lr=0.0)\n",
    "opt.param_groups[-1]['lr'] = 0.01\n",
    "\n",
    "# Make data\n",
    "# @TODO: make this code compatible with one dataset (no dann)\n",
    "data_fn = partial(utils.DomainAgnosticSortishSampler, _batchsize=bs, _padidx=1)\n",
    "data_train = [{'x': trn_texts_a, 'y': trn_labels_a}, {'x': trn_texts_b, 'y': trn_labels_b}]\n",
    "data_valid = [{'x': val_texts_a, 'y': val_labels_a}, {'x': val_texts_b, 'y': val_labels_b}]\n",
    "data = {'train': data_train, 'valid': data_valid}\n",
    "\n",
    "# Make lr scheduler\n",
    "lr_args = {'iterations': len(data_fn(data_train)), 'cycles': 1}\n",
    "lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.CosineAnnealingLR)\n",
    "\n",
    "save_args = {'torch_stuff': [tosave('model.torch', clf.state_dict()), tosave('model_enc.torch', clf.encoder.state_dict())]}\n",
    "save_fnames = {'torch_stuff':\n",
    "                   {'hightrn':\n",
    "                        {'model': 'sup_model_hightrn.torch',\n",
    "                         'enc': 'sup_model_hightrn_enc.torch'},\n",
    "                    'lowaux':\n",
    "                        {'model': 'sup_model_lowaux.torch',\n",
    "                         'enc': 'sup_model_lowaux_enc.torch'}}}\n",
    "\n",
    "args = {'epochs': 1, 'epoch_count': 0, 'data': data, 'device': device, 'opt': opt,\n",
    "        'loss_main_fn': loss_main_fn, 'loss_aux_fn': loss_aux_fn, 'model': clf,\n",
    "        'train_fn': clf, 'predict_fn': clf.predict, 'train_aux_fn': clf.domain,\n",
    "        'epoch_end_hook': epoch_end_hook, 'weight_decay': params.weight_decay,\n",
    "        'clip_grads_at': params.clip_grads_at, 'lr_schedule': lr_schedule,\n",
    "        'loss_scale': params.loss_scale if len(DATASETS) > 1 else 0,\n",
    "        'data_fn': data_fn, 'eval_fn': _eval, 'eval_aux_fn': _eval,\n",
    "        'save': not SAFE_MODE, 'save_params': params, 'save_dir': UNSUP_MODEL_DIR, 'save_fnames': save_fnames}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Training schedule:\n",
    "    \n",
    "    1. Unfreeze one layer. Train for 1 epoch\n",
    "    2 - 5. Unfreeze one layer, train for 1 epoch\n",
    "    3. Train for 15 epochs (after all layers are unfrozen). Use 15 cycles for cosine annealing.\n",
    "'''\n",
    "# opt.param_groups[-1]['lr'] = 0.01\n",
    "traces = utils.dann_loop(**args)\n",
    "\n",
    "opt.param_groups[-1]['lr'] = 0.01\n",
    "opt.param_groups[-2]['lr'] = 0.005\n",
    "lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.CosineAnnealingLR)\n",
    "args['lr_schedule'] = lr_schedule\n",
    "args['save_above'] = np.max(traces[TRACES_FORMAT['train_acc']])\n",
    "args['epoch_count'] += 1\n",
    "traces_new = utils.dann_loop(**args)\n",
    "traces = [a+b for a, b in zip(traces, traces_new)]\n",
    "\n",
    "opt.param_groups[-1]['lr'] = 0.01\n",
    "opt.param_groups[-2]['lr'] = 0.005\n",
    "opt.param_groups[-3]['lr'] = 0.001\n",
    "lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.CosineAnnealingLR)\n",
    "args['lr_schedule'] = lr_schedule\n",
    "args['save_above'] = np.max(traces[TRACES_FORMAT['train_acc']])\n",
    "args['epoch_count'] += 1\n",
    "traces_new = utils.dann_loop(**args)\n",
    "traces = [a+b for a, b in zip(traces, traces_new)]\n",
    "\n",
    "opt.param_groups[-1]['lr'] = 0.01\n",
    "opt.param_groups[-2]['lr'] = 0.005\n",
    "opt.param_groups[-3]['lr'] = 0.001\n",
    "opt.param_groups[-4]['lr'] = 0.001\n",
    "lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.CosineAnnealingLR)\n",
    "args['lr_schedule'] = lr_schedule\n",
    "args['save_above'] = np.max(traces[TRACES_FORMAT['train_acc']])\n",
    "args['epoch_count'] += 1\n",
    "traces_new = utils.dann_loop(**args)\n",
    "traces = [a+b for a, b in zip(traces, traces_new)]\n",
    "\n",
    "opt.param_groups[-1]['lr'] = 0.01\n",
    "opt.param_groups[-2]['lr'] = 0.005\n",
    "opt.param_groups[-3]['lr'] = 0.001\n",
    "opt.param_groups[-4]['lr'] = 0.001\n",
    "opt.param_groups[-5]['lr'] = 0.001\n",
    "lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.CosineAnnealingLR)\n",
    "args['lr_schedule'] = lr_schedule\n",
    "args['save_above'] = np.max(traces[TRACES_FORMAT['train_acc']])\n",
    "args['epoch_count'] += 1\n",
    "traces_new = utils.dann_loop(**args)\n",
    "traces = [a+b for a, b in zip(traces, traces_new)]\n",
    "\n",
    "opt.param_groups[-1]['lr'] = 0.01\n",
    "opt.param_groups[-2]['lr'] = 0.005\n",
    "opt.param_groups[-3]['lr'] = 0.001\n",
    "opt.param_groups[-4]['lr'] = 0.001\n",
    "opt.param_groups[-5]['lr'] = 0.001\n",
    "lr_args['cycles'] = 15\n",
    "args['epochs'] = 15\n",
    "lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.CosineAnnealingLR)\n",
    "args['lr_schedule'] = lr_schedule\n",
    "args['save_above'] = np.max(traces[TRACES_FORMAT['train_acc']])\n",
    "args['epoch_count'] += 1\n",
    "args['notify'] = True\n",
    "\n",
    "traces_new = utils.dann_loop(**args)\n",
    "traces = [a+b for a, b in zip(traces, traces_new)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SAFE_MODE:\n",
    "    mt_save(UNSUP_MODEL_DIR,\n",
    "            torch_stuff=[tosave('sup_model_final.torch', clf.state_dict())],\n",
    "            pickle_stuff=[tosave('final_sup_traces.pkl', traces), tosave('unsup_options.pkl', params)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNSUP_MODEL_DIR"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot(trcs):\n",
    "    layers = len(trcs[0])\n",
    "    for l in range(layers):\n",
    "        plt.plot(trcs[:,l], label=f\"layer {l}\")\n",
    "    plt.show()\n",
    "    \n",
    "plot(np.asarray(traces[-1][100:]))\n",
    "plot(np.asarray([[x[-1]] for x in traces[-1][:]]))\n",
    "\n",
    "print(lr_args)\n",
    "lr_args['cycles'] = 5\n",
    "lr_args['iterations'] = 42*15\n",
    "lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.CosineAnnealingLR)\n",
    "lrs = []\n",
    "while True:\n",
    "    try:\n",
    "        lrs.append(lr_schedule.get())\n",
    "    except CustomError:\n",
    "        break\n",
    "plot(np.asarray(lrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style as pltstyle\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (16, 8)\n",
    "\n",
    "def plot_accs(tra, vla, style=None, save_dir=None):\n",
    "    pltstyle.use(style if style else 'seaborn-deep')\n",
    "    fig = plt.figure(figsize = (16,8))\n",
    "    ax = fig.add_axes((0.1, 0.2, 0.8, 0.7))\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "    plt.plot(tra, label=f\"Train Acc\", linewidth=3)\n",
    "    plt.plot(vla, label=f\"Valid Acc\", linewidth=3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    if not save_dir is None:\n",
    "        # Dumping the plot as well\n",
    "        pltstyle.use(style if style else 'seaborn-deep')\n",
    "        fig = plt.figure(figsize = (16,8))\n",
    "        ax = fig.add_axes((0.1, 0.2, 0.8, 0.7))\n",
    "        ax.spines['right'].set_color('none')\n",
    "        ax.spines['top'].set_color('none')\n",
    "    #     plt.xticks([])\n",
    "    #     plt.yticks([])\n",
    "        plt.plot(tra, label=f\"Train Acc\", linewidth=3)\n",
    "        plt.plot(vla, label=f\"Valid Acc\", linewidth=3)\n",
    "        plt.legend()\n",
    "        plt.savefig(save_dir/'sup_acc.png')\n",
    "    \n",
    "def plot(trcs):\n",
    "    layers = len(trcs[0])\n",
    "    for l in range(layers):\n",
    "        plt.plot(trcs[:,l], label=f\"layer {l}\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_accs(traces[0], traces[1], save_dir=UNSUP_MODEL_DIR if not SAFE_MODE else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_best = np.max(traces[0])\n",
    "trn_best_ = np.argmax(traces[0])\n",
    "val_attrn = traces[1][trn_best_]\n",
    "val_best = np.max(traces[1])\n",
    "val_best_ = np.argmax(traces[1])\n",
    "print(f\"Train Best: {trn_best:.4f} at {trn_best_}\\nValid @Trn: {val_attrn:.4f}\\nValid Best: {val_best:.4f} at {val_best_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
