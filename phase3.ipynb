{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# pretty print all cell's output and not just the last one\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     58,
     66
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Pulls an unsupervised fine tuned model from disk, also data, and goes to town on it.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f598baed270>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\n    Paths and macros\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\n    Model code\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\n    Prepare data\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Pulls an unsupervised fine tuned model from disk, also data, and goes to town on it.\n",
    "\"\"\"\n",
    "\n",
    "import html\n",
    "import os\n",
    "import pickle\n",
    "# External Lib imports\n",
    "import re\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['QT_QPA_PLATFORM'] = 'offscreen'\n",
    "\n",
    "# FastAI Imports\n",
    "from fastai import text, core, lm_rnn\n",
    "\n",
    "# Torch imports\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Mytorch imports\n",
    "from mytorch import loops, lriters as mtlr, dataiters as mtdi\n",
    "from mytorch.utils.goodies import *\n",
    "\n",
    "device = torch.device('cuda')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "'''\n",
    "    Paths and macros\n",
    "'''\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "# Path fields\n",
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "\n",
    "DATA_PATH = Path('raw/imdb/aclImdb/')\n",
    "DATA_PATH.mkdir(exist_ok=True)\n",
    "PATH = Path('resources/proc/imdb')\n",
    "DATA_PROC_PATH = PATH / 'data'\n",
    "DATA_LM_PATH = PATH / 'datalm'\n",
    "\n",
    "LM_PATH = Path('resources/models')\n",
    "LM_PATH.mkdir(exist_ok=True)\n",
    "PRE_PATH = LM_PATH / 'wt103'\n",
    "PRE_LM_PATH = PRE_PATH / 'fwd_wt103.h5'\n",
    "CLASSES = ['neg', 'pos', 'unsup']\n",
    "\n",
    "'''\n",
    "    Model code\n",
    "'''\n",
    "\n",
    "\n",
    "class CustomEncoder(lm_rnn.MultiBatchRNN):\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return torch.nn.ModuleList([torch.nn.ModuleList([self.rnns[0], self.dropouths[0]]),\n",
    "                                    torch.nn.ModuleList([self.rnns[1], self.dropouths[1]]),\n",
    "                                    torch.nn.ModuleList([self.rnns[2], self.dropouths[2]])])\n",
    "\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "\n",
    "    @staticmethod\n",
    "    def freeze_layer(layer):\n",
    "        for params in layer.parameters():\n",
    "            params.requires_grad = False\n",
    "\n",
    "    @staticmethod\n",
    "    def unfreeze_layer(layer):\n",
    "        for params in layer.parameters():\n",
    "            params.requires_grad = True\n",
    "\n",
    "    # @TODO: inject comments.\n",
    "    def __init__(self, _device: torch.device, ntoken: int, dps: list, enc_wgts, _debug=False):\n",
    "        super(TextClassifier, self).__init__()\n",
    "\n",
    "        self.device = _device\n",
    "\n",
    "        # Load the pre-trained model\n",
    "        args = {'ntoken': ntoken, 'emb_sz': 400, 'n_hid': 1150,\n",
    "                'n_layers': 3, 'pad_token': 0, 'qrnn': False, 'bptt': 70, 'max_seq': 1400,\n",
    "                'dropouti': dps[0], 'wdrop': dps[1], 'dropoute': dps[2], 'dropouth': dps[3]}\n",
    "        self.encoder = CustomEncoder(**args).to(self.device)\n",
    "        self.encoder.load_state_dict(enc_wgts)\n",
    "        '''\n",
    "            Make new classifier.\n",
    "            \n",
    "            Explanation:\n",
    "                400*3 because input is [ h_T, maxpool, meanpool ]\n",
    "                50 is hidden layer dim\n",
    "                2 is n_classes\n",
    "\n",
    "                0.4, 0.1 are drops at various layers\n",
    "        '''\n",
    "        self.linear = text.PoolingLinearClassifier(layers=[400 * 3, 50, 2], drops=[dps[4], 0.1]).to(self.device)\n",
    "        self.encoder.reset()\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        layers = [x for x in self.encoder.layers]\n",
    "        layers += [x for x in self.linear.layers]\n",
    "        return torch.nn.ModuleList(layers)\n",
    "\n",
    "    @property\n",
    "    def layers_rev(self):\n",
    "        layers = [x for x in self.encoder.layers]\n",
    "        layers += [x for x in self.linear.layers]\n",
    "        layers.reverse()\n",
    "        return torch.nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # inputs are S*B\n",
    "\n",
    "        # Encoding all the data\n",
    "        op_p = self.encoder(x.transpose(1, 0))\n",
    "        # pos_batch = op_p[1][-1][-1]\n",
    "        score = self.linear(op_p)[0]\n",
    "\n",
    "        return score\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "            Same code works for both pairwise or pointwise\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            predicted = self.forward(x)\n",
    "            self.train()\n",
    "            return predicted\n",
    "\n",
    "\n",
    "'''\n",
    "    Prepare data\n",
    "'''\n",
    "re1 = re.compile(r'  +')\n",
    "\n",
    "\n",
    "def fixup(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
    "        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "\n",
    "def get_texts(df, n_lbls=1):\n",
    "#     print(df)\n",
    "    labels = df.iloc[:, range(n_lbls)].values.astype(np.int64)\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df.iloc[:, 1].astype(str)\n",
    "    texts = list(texts.apply(fixup).values)\n",
    "\n",
    "\n",
    "    tok = text.Tokenizer().proc_all_mp(core.partition_by_cores(texts))\n",
    "    return tok, list(labels)\n",
    "\n",
    "\n",
    "def get_all(df, n_lbls):\n",
    "    tok, labels = get_texts(df)\n",
    "    return tok, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_org(path):\n",
    "    texts, labels = [], []\n",
    "    for idx, label in enumerate(CLASSES):\n",
    "        for fname in (path / label).glob('*.*'):\n",
    "            texts.append(fname.open('r', encoding='utf-8').read())\n",
    "            labels.append(idx)\n",
    "    return np.array(texts), np.array(labels)\n",
    "\n",
    "trn_texts, trn_labels = get_texts_org(DATA_PATH / 'train')\n",
    "val_texts, val_labels = get_texts_org(DATA_PATH / 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lose label 2 from train\n",
    "trn_texts = trn_texts[trn_labels<2]\n",
    "trn_labels = trn_labels[trn_labels<2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "np.random.seed(42)\n",
    "trn_idx = np.random.permutation(len(trn_texts))[:1000]\n",
    "val_idx = np.random.permutation(len(val_texts))[:1000]\n",
    "\n",
    "trn_texts, trn_labels = trn_texts[trn_idx], trn_labels[trn_idx]\n",
    "val_texts, val_labels = val_texts[val_idx], val_labels[val_idx]\n",
    "col_names = ['labels', 'text']\n",
    "\n",
    "df_trn = pd.DataFrame({'text': trn_texts, 'labels': trn_labels}, columns=col_names)\n",
    "df_val = pd.DataFrame({'text': val_texts, 'labels': val_labels}, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "if DANN: itos_path = LM_PATH / 'tmp' / 'itos.pkl'\n",
    "else: itos_path = DATA_LM_PATH / 'tmp' / 'itos.pkl'\n",
    "itos2 = pickle.load(itos_path.open('rb'))\n",
    "# stoi2 = collections.defaultdict(lambda: 0, {v: k for k, v in enumerate(itos2)})\n",
    "stoi2 = {v: k for k, v in enumerate(itos2)}\n",
    "\n",
    "# chunksize = 24000\n",
    "# df_trn_csv = pd.read_csv(DATA_PROC_PATH / 'train.csv', header=None, chunksize=chunksize)\n",
    "# df_val_csv = pd.read_csv(DATA_PROC_PATH / 'test.csv', header=None, chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_clas, trn_labels = get_all(df_trn, 1)\n",
    "val_clas, val_labels = get_all(df_val, 1)\n",
    "\n",
    "trn_clas = np.array([[stoi2.get(w, 0) for w in para] for para in trn_clas])\n",
    "val_clas = np.array([[stoi2.get(w, 0) for w in para] for para in val_clas])\n",
    "trn_labels = [x for y in trn_labels for x in y]\n",
    "val_labels = [x for y in val_labels for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9287"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itos2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " 'xbos',\n",
       " 'xfld',\n",
       " '1',\n",
       " 'i',\n",
       " 'went',\n",
       " 'into',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'thinking',\n",
       " 'that',\n",
       " 'it',\n",
       " 'would',\n",
       " 'be',\n",
       " 'a',\n",
       " 'neat',\n",
       " 'football',\n",
       " 'drama',\n",
       " '(',\n",
       " 'in',\n",
       " 'the',\n",
       " 'same',\n",
       " 'vein',\n",
       " 'as',\n",
       " 'remember',\n",
       " 'the',\n",
       " '_unk_',\n",
       " ')',\n",
       " ';',\n",
       " 'however',\n",
       " ',',\n",
       " 'i',\n",
       " 'came',\n",
       " 'away',\n",
       " 'feeling',\n",
       " 'like',\n",
       " 'i',\n",
       " 'had',\n",
       " 'just',\n",
       " 'attended',\n",
       " 'a',\n",
       " 'ted',\n",
       " '_unk_',\n",
       " '_unk_',\n",
       " 'about',\n",
       " 'the',\n",
       " 'rapture',\n",
       " '.',\n",
       " 'the',\n",
       " 'only',\n",
       " 'thing',\n",
       " 'that',\n",
       " 'was',\n",
       " 'missing',\n",
       " 'was',\n",
       " 'the',\n",
       " '_unk_',\n",
       " 'for',\n",
       " 'a',\n",
       " '_unk_',\n",
       " 'at',\n",
       " 'the',\n",
       " 'end',\n",
       " 'of',\n",
       " 'the',\n",
       " 'movie',\n",
       " '.',\n",
       " 'actually',\n",
       " ',',\n",
       " 'one',\n",
       " 'would',\n",
       " 'probably',\n",
       " 'get',\n",
       " 'more',\n",
       " 'out',\n",
       " 'of',\n",
       " 'a',\n",
       " '_unk_',\n",
       " '_unk_',\n",
       " 'than',\n",
       " 'this',\n",
       " 'poor',\n",
       " 'excuse',\n",
       " 'for',\n",
       " 'entertainment',\n",
       " '.',\n",
       " 'at',\n",
       " 'least',\n",
       " 'with',\n",
       " 'the',\n",
       " '_unk_',\n",
       " '_unk_',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'quotes',\n",
       " 'from',\n",
       " 'the',\n",
       " '_unk_',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'quotes',\n",
       " 'from',\n",
       " '_unk_',\n",
       " 'self',\n",
       " '-',\n",
       " 'help',\n",
       " 'pulp',\n",
       " '.',\n",
       " 'the',\n",
       " 'plot',\n",
       " 'was',\n",
       " 'entirely',\n",
       " 'too',\n",
       " 'predictable',\n",
       " 'to',\n",
       " 'the',\n",
       " 'point',\n",
       " 'that',\n",
       " 'anyone',\n",
       " 'with',\n",
       " 'a',\n",
       " 'long',\n",
       " '-',\n",
       " 'enough',\n",
       " 'attention',\n",
       " 'span',\n",
       " 'could',\n",
       " 'have',\n",
       " 'laid',\n",
       " 'out',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'plot',\n",
       " 'within',\n",
       " 'the',\n",
       " 'first',\n",
       " '15',\n",
       " 'minutes',\n",
       " 'of',\n",
       " 'the',\n",
       " 'movie',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[itos2[x] for x in trn_clas[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Make model\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\n    Setup things for training (data, loss, opt, lr schedule etc\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Make model\n",
    "'''\n",
    "dps = list(np.asarray([0.4, 0.5, 0.05, 0.3, 0.4]) * 0.5)\n",
    "# enc_wgts = torch.load(LM_PATH, map_location=lambda storage, loc: storage)\n",
    "enc_wgts = torch.load(PATH / 'unsup_model_enc.torch', map_location=lambda storage, loc: storage)\n",
    "clf = TextClassifier(device, len(itos2), dps, enc_wgts)\n",
    "\n",
    "'''\n",
    "    Setup things for training (data, loss, opt, lr schedule etc\n",
    "'''\n",
    "bs = 24\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "opt = make_opt(clf, opt_fn, lr=0.0)\n",
    "opt.param_groups[-1]['lr'] = 0.01\n",
    "\n",
    "# Make data\n",
    "data_fn = partial(mtdi.SortishSampler, _batchsize=bs, _padidx=1)\n",
    "data = {'train': {'x': trn_clas, 'y': trn_labels}, 'valid': {'x': val_clas, 'y': val_labels}}\n",
    "\n",
    "# Make lr scheduler\n",
    "lr_args = {'iterations': len(data_fn(data['train'])), 'cycles': 1}\n",
    "lr_schedule = mtlr.LearningRateScheduler(opt, lr_args, mtlr.CosineAnnealingLR)\n",
    "\n",
    "\n",
    "def epoch_end_hook() -> None:\n",
    "    lr_schedule.reset()\n",
    "\n",
    "\n",
    "def eval(y_pred, y_true):\n",
    "    \"\"\"\n",
    "        Expects a batch of input\n",
    "\n",
    "        :param y_pred: tensor of shape (b, nc)\n",
    "        :param y_true: tensor of shape (b, 1)\n",
    "    \"\"\"\n",
    "    return torch.mean((torch.argmax(y_pred, dim=1) == y_true).float())\n",
    "\n",
    "\n",
    "args = {'epochs': 1, 'data': data, 'device': device,\n",
    "        'opt': opt, 'loss_fn': loss_fn, 'model': clf,\n",
    "        'train_fn': clf, 'predict_fn': clf.predict,\n",
    "        'epoch_end_hook': epoch_end_hook, 'weight_decay': 1e-7,\n",
    "        'clip_grads_at': 0.30, 'lr_schedule': lr_schedule,\n",
    "        'data_fn': data_fn, 'eval_fn': eval}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x,y = data_fn(data['train']).__next__()\n",
    "x,y = torch.tensor(x, dtype=torch.long, device=device), torch.tensor(y, dtype=torch.long, device=device)\n",
    "y_pred = torch.randn(y.shape[0],2, device=device)\n",
    "x.shape, y.shape, y_pred.shape, x.device, y.device, y_pred.device\n",
    "loss_fn(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = loops.generic_loop(**args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.param_groups[-2]['lr'] = 0.001\n",
    "lr_schedule = mtlr.LearningRateScheduler(opt, lr_args, mtlr.CosineAnnealingLR)\n",
    "args['lr_schedule'] = lr_schedule\n",
    "traces_new = loops.generic_loop(**args)\n",
    "traces = [a+b for a, b in zip(traces, traces_new)]\n",
    "\n",
    "opt.param_groups[-3]['lr'] = 0.0001\n",
    "lr_schedule = mtlr.LearningRateScheduler(opt, lr_args, mtlr.CosineAnnealingLR)\n",
    "args['lr_schedule'] = lr_schedule\n",
    "traces_new = loops.generic_loop(**args)\n",
    "traces = [a+b for a, b in zip(traces, traces_new)]\n",
    "\n",
    "opt.param_groups[-4]['lr'] = 0.0001\n",
    "lr_schedule = mtlr.LearningRateScheduler(opt, lr_args, mtlr.CosineAnnealingLR)\n",
    "args['lr_schedule'] = lr_schedule\n",
    "traces_new = loops.generic_loop(**args)\n",
    "traces = [a+b for a, b in zip(traces, traces_new)]\n",
    "\n",
    "opt.param_groups[-5]['lr'] = 0.0001\n",
    "lr_schedule = mtlr.LearningRateScheduler(opt, lr_args, mtlr.CosineAnnealingLR)\n",
    "args['lr_schedule'] = lr_schedule\n",
    "traces_new = loops.generic_loop(**args)\n",
    "traces = [a+b for a, b in zip(traces, traces_new)]\n",
    "\n",
    "lr_args['cycles'] = 15\n",
    "args['epochs'] = 15\n",
    "lr_schedule = mtlr.LearningRateScheduler(opt, lr_args, mtlr.CosineAnnealingLR)\n",
    "args['lr_schedule'] = lr_schedule\n",
    "traces_new = loops.generic_loop(**args)\n",
    "traces = [a+b for a, b in zip(traces, traces_new)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping the traces\n",
    "with open(PATH/'sup_traces.pkl' if not DANN else 'sup_dann_traces.pkl', 'wb+') as fl:\n",
    "    pickle.dump(traces, fl)\n",
    "    \n",
    "# Dumping the model\n",
    "torch.save(clf.state_dict(), PATH / 'sup_model.torch' if not DANN else 'sup_dann_model.torch')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def plot(trcs):\n",
    "    layers = len(trcs[0])\n",
    "    for l in range(layers):\n",
    "        plt.plot(trcs[:,l], label=f\"layer {l}\")\n",
    "    plt.show()\n",
    "\n",
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "opt = make_opt(clf, opt_fn, lr=0.01)\n",
    "opt.param_groups[-1]['lr'] = 0.01\n",
    "# lr_args = {'iterations': len(data_fn(data['train'])), 'cycles': 1}\n",
    "lr_args = {'length': len(data_fn(data['train']))}\n",
    "lr_schedule = mtlr.LearningRateScheduler(opt, lr_args, mtlr.ConstantLR)\n",
    "lrss = [lr_schedule.get() for _ in range(lr_args['length'])]\n",
    "plot(np.asarray(lrss))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lr_args = {'iterations': len(data_fn(data['train'])), 'cycles': 1}\n",
    "lr_schedule = mtlr.LearningRateScheduler(opt, lr_args, mtlr.CosineAnnealingLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(trcs):\n",
    "    layers = len(trcs[0])\n",
    "    for l in range(layers):\n",
    "        plt.plot(trcs[:,l], label=f\"layer {l}\")\n",
    "    plt.show()\n",
    "    \n",
    "plot(np.asarray(traces[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
