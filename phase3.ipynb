{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# pretty print all cell's output and not just the last one\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# External Lib imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "\n",
    "os.environ['QT_QPA_PLATFORM'] = 'offscreen'\n",
    "\n",
    "# FastAI Imports\n",
    "from fastai import text, lm_rnn\n",
    "\n",
    "# Torch imports\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Mytorch imports\n",
    "from mytorch import loops, lriters as mtlr, dataiters as mtdi\n",
    "from mytorch.utils.goodies import *\n",
    "\n",
    "# Local imports\n",
    "from data import DataPuller\n",
    "from options import Phase3 as params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations for this run\n",
    "QUICK = False\n",
    "DEBUG = True\n",
    "MODEL_NUM = 3\n",
    "PRETRAINED = True\n",
    "MODEL_SUFFIX = ''\n",
    "SAFE_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0.0,
     58.0,
     66.0
    ]
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "'''\n",
    "    Paths and macros\n",
    "'''\n",
    "PATH = Path('resources/proc/imdb')\n",
    "DATA_PROC_PATH = PATH / 'data'\n",
    "DATA_LM_PATH = PATH / 'datalm'\n",
    "\n",
    "LM_PATH = Path('resources/models')\n",
    "LM_PATH.mkdir(exist_ok=True)\n",
    "PRE_PATH = LM_PATH / 'wt103'\n",
    "PRE_LM_PATH = PRE_PATH / 'fwd_wt103.h5'\n",
    "\n",
    "\n",
    "'''\n",
    "    Model code\n",
    "'''\n",
    "\n",
    "\n",
    "class CustomEncoder(lm_rnn.MultiBatchRNN):\n",
    "    @property\n",
    "    def layers(self):\n",
    "        # TODO: ADD ENCODERR!!!!!!!!!!\n",
    "        return torch.nn.ModuleList([torch.nn.ModuleList([self.rnns[0], self.dropouths[0]]),\n",
    "                                    torch.nn.ModuleList([self.rnns[1], self.dropouths[1]]),\n",
    "                                    torch.nn.ModuleList([self.rnns[2], self.dropouths[2]])])\n",
    "\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "\n",
    "    # @TODO: inject comments.\n",
    "    def __init__(self,\n",
    "                 _device: torch.device,\n",
    "                 ntoken: int,\n",
    "                 dps: list,\n",
    "                 enc_wgts = None,\n",
    "                 _debug=False):\n",
    "        super(TextClassifier, self).__init__()\n",
    "\n",
    "        self.device = _device\n",
    "\n",
    "        # Load the pre-trained model\n",
    "        args = {'ntoken': ntoken, 'emb_sz': 400, 'n_hid': 1150,\n",
    "                'n_layers': 3, 'pad_token': 0, 'qrnn': False, 'bptt': 70, 'max_seq': 1400,\n",
    "                'dropouti': dps[0], 'wdrop': dps[1], 'dropoute': dps[2], 'dropouth': dps[3]}\n",
    "        self.encoder = CustomEncoder(**args).to(self.device)\n",
    "        if enc_wgts:\n",
    "            self.encoder.load_state_dict(enc_wgts)\n",
    "        '''\n",
    "            Make new classifier.\n",
    "            \n",
    "            Explanation:\n",
    "                400*3 because input is [ h_T, maxpool, meanpool ]\n",
    "                50 is hidden layer dim\n",
    "                2 is n_classes\n",
    "\n",
    "                0.4, 0.1 are drops at various layers\n",
    "        '''\n",
    "        self.linear = text.PoolingLinearClassifier(layers=[400 * 3, 50, 2], drops=[dps[4], 0.1]).to(self.device)\n",
    "        self.encoder.reset()\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        layers = [x for x in self.encoder.layers]\n",
    "        layers += [x for x in self.linear.layers]\n",
    "        return torch.nn.ModuleList(layers)\n",
    "\n",
    "    @property\n",
    "    def layers_rev(self):\n",
    "        layers = [x for x in self.encoder.layers]\n",
    "        layers += [x for x in self.linear.layers]\n",
    "        layers.reverse()\n",
    "        return torch.nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # inputs are S*B\n",
    "\n",
    "        # Encoding all the data\n",
    "        op_p = self.encoder(x.transpose(1, 0))\n",
    "        # pos_batch = op_p[1][-1][-1]\n",
    "        score = self.linear(op_p)[0]\n",
    "\n",
    "        return score\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            predicted = self.forward(x)\n",
    "            self.train()\n",
    "            return predicted\n",
    "\n",
    "\n",
    "def epoch_end_hook() -> None:\n",
    "    lr_schedule.reset()\n",
    "\n",
    "\n",
    "# noinspection PyUnresolvedReferences\n",
    "def _eval(y_pred, y_true):\n",
    "    \"\"\"\n",
    "        Expects a batch of input\n",
    "\n",
    "        :param y_pred: tensor of shape (b, nc)\n",
    "        :param y_true: tensor of shape (b, 1)\n",
    "    \"\"\"\n",
    "    return torch.mean((torch.argmax(y_pred, dim=1) == y_true).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNSUP_MODEL_DIR = PATH / 'models' / MODEL_NUM\n",
    "assert MODEL_SUFFIX in ['_lowaux', '_hightrn', '', '_final'], 'Incorrect Suffix given with which to load model'\n",
    "\n",
    "params.quick = QUICK\n",
    "params.model_dir = str(UNSUP_MODEL_DIR) + ' and ' + str(MODEL_NUM)\n",
    "params.model_suffix = MODEL_SUFFIX\n",
    "\n",
    "data_puller = DataPuller(debug=False, max_vocab=params.max_vocab_task, min_freq=params.min_vocab_freq, trim_trn=1000, trim_val=-1)\n",
    "trn_texts, trn_labels, val_texts, val_labels, itos = data_puller.get('imdb', supervised=True, trim=params.quick)\n",
    "\n",
    "# Lose label 2 from train\n",
    "trn_texts = trn_texts[trn_labels < 2]\n",
    "trn_labels = trn_labels[trn_labels < 2]\n",
    "\n",
    "# Create representations of text using old itos\n",
    "itos_path = UNSUP_MODEL_DIR / 'itos.pkl'\n",
    "itos2 = pickle.load(itos_path.open('rb'))\n",
    "stoi2 = {v: k for k, v in enumerate(itos2)}\n",
    "\n",
    "trn_texts = np.array([[stoi2.get(w, 0) for w in para] for para in trn_texts])\n",
    "val_texts = np.array([[stoi2.get(w, 0) for w in para] for para in val_texts])\n",
    "\n",
    "'''\n",
    "    Make model\n",
    "'''\n",
    "dps = list(params.encoder_dropouts)\n",
    "# enc_wgts = torch.load(LM_PATH, map_location=lambda storage, loc: storage)\n",
    "enc_wgts = torch.load(UNSUP_MODEL_DIR / ('unsup_model_enc' + MODEL_SUFFIX + '.torch'), map_location=lambda storage, loc: storage)\n",
    "clf = TextClassifier(device, len(itos2), dps, enc_wgts=enc_wgts if PRETRAINED else None)\n",
    "\n",
    "'''\n",
    "    Setup things for training (data, loss, opt, lr schedule etc\n",
    "'''\n",
    "bs = params.bs\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "opt_fn = partial(optim.Adam, betas=params.adam_betas)\n",
    "opt = make_opt(clf, opt_fn, lr=0.0)\n",
    "opt.param_groups[-1]['lr'] = 0.01\n",
    "\n",
    "# Make data\n",
    "data_fn = partial(mtdi.SortishSampler, _batchsize=bs, _padidx=1)\n",
    "data = {'train': {'x': trn_texts, 'y': trn_labels}, 'valid': {'x': val_texts, 'y': val_labels}}\n",
    "\n",
    "# Make lr scheduler\n",
    "lr_args = {'iterations': len(data_fn(data['train'])), 'cycles': 1}\n",
    "lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.CosineAnnealingLR)\n",
    "\n",
    "save_args = {'torch_stuff': [tosave('model.torch', clf.state_dict()), tosave('model_enc.torch', clf.encoder.state_dict())]}\n",
    "\n",
    "args = {'epochs': 1, 'epoch_count':0, 'data': data, 'device': device,\n",
    "        'opt': opt, 'loss_fn': loss_fn, 'model': clf,\n",
    "        'train_fn': clf, 'predict_fn': clf.predict,\n",
    "        'epoch_end_hook': epoch_end_hook, 'weight_decay': params.weight_decay,\n",
    "        'clip_grads_at': params.clip_grads_at, 'lr_schedule': lr_schedule,\n",
    "        'data_fn': data_fn, 'eval_fn': _eval,\n",
    "        'save': not SAFE_MODE, 'save_params': params, 'save_dir': UNSUP_MODEL_DIR, 'save_args': save_args}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Training schedule:\n",
    "    \n",
    "    1. Unfreeze one layer. Train for 1 epoch\n",
    "    2 - 5. Unfreeze one layer, train for 1 epoch\n",
    "    3. Train for 15 epochs (after all layers are unfrozen). Use 15 cycles for cosine annealing.\n",
    "'''\n",
    "# opt.param_groups[-1]['lr'] = 0.01\n",
    "traces = loops.generic_loop(**args)\n",
    "\n",
    "opt.param_groups[-1]['lr'] = 0.01\n",
    "opt.param_groups[-2]['lr'] = 0.005\n",
    "lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.CosineAnnealingLR)\n",
    "args['lr_schedule'] = lr_schedule\n",
    "traces_new = loops.generic_loop(**args)\n",
    "traces = [a+b for a, b in zip(traces, traces_new)]\n",
    "\n",
    "opt.param_groups[-1]['lr'] = 0.01\n",
    "opt.param_groups[-2]['lr'] = 0.005\n",
    "opt.param_groups[-3]['lr'] = 0.001\n",
    "lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.CosineAnnealingLR)\n",
    "args['lr_schedule'] = lr_schedule\n",
    "traces_new = loops.generic_loop(**args)\n",
    "traces = [a+b for a, b in zip(traces, traces_new)]\n",
    "\n",
    "opt.param_groups[-1]['lr'] = 0.01\n",
    "opt.param_groups[-2]['lr'] = 0.005\n",
    "opt.param_groups[-3]['lr'] = 0.001\n",
    "opt.param_groups[-4]['lr'] = 0.001\n",
    "lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.CosineAnnealingLR)\n",
    "args['lr_schedule'] = lr_schedule\n",
    "traces_new = loops.generic_loop(**args)\n",
    "traces = [a+b for a, b in zip(traces, traces_new)]\n",
    "\n",
    "opt.param_groups[-1]['lr'] = 0.01\n",
    "opt.param_groups[-2]['lr'] = 0.005\n",
    "opt.param_groups[-3]['lr'] = 0.001\n",
    "opt.param_groups[-4]['lr'] = 0.001\n",
    "opt.param_groups[-5]['lr'] = 0.001\n",
    "lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.CosineAnnealingLR)\n",
    "args['lr_schedule'] = lr_schedule\n",
    "traces_new = loops.generic_loop(**args)\n",
    "traces = [a+b for a, b in zip(traces, traces_new)]\n",
    "\n",
    "opt.param_groups[-1]['lr'] = 0.01\n",
    "opt.param_groups[-2]['lr'] = 0.005\n",
    "opt.param_groups[-3]['lr'] = 0.001\n",
    "opt.param_groups[-4]['lr'] = 0.001\n",
    "opt.param_groups[-5]['lr'] = 0.001\n",
    "lr_args['cycles'] = 15\n",
    "args['epochs'] = 15\n",
    "lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.CosineAnnealingLR)\n",
    "args['lr_schedule'] = lr_schedule\n",
    "args['notify'] = True\n",
    "\n",
    "traces_new = loops.generic_loop(**args)\n",
    "traces = [a+b for a, b in zip(traces, traces_new)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SAFE_MODE:\n",
    "    mt_save(UNSUP_MODEL_DIR,\n",
    "            torch_stuff=[tosave('sup_model_final.torch', clf.state_dict())],\n",
    "            pickle_stuff=[tosave('final_sup_traces.pkl', traces), tosave('unsup_options.pkl', params)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNSUP_MODEL_DIR"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot(trcs):\n",
    "    layers = len(trcs[0])\n",
    "    for l in range(layers):\n",
    "        plt.plot(trcs[:,l], label=f\"layer {l}\")\n",
    "    plt.show()\n",
    "    \n",
    "plot(np.asarray(traces[-1][100:]))\n",
    "plot(np.asarray([[x[-1]] for x in traces[-1][:]]))\n",
    "\n",
    "print(lr_args)\n",
    "lr_args['cycles'] = 5\n",
    "lr_args['iterations'] = 42*15\n",
    "lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.CosineAnnealingLR)\n",
    "lrs = []\n",
    "while True:\n",
    "    try:\n",
    "        lrs.append(lr_schedule.get())\n",
    "    except CustomError:\n",
    "        break\n",
    "plot(np.asarray(lrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style as pltstyle\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (16, 8)\n",
    "\n",
    "def plot_accs(tra, vla, style=None, save_dir=None):\n",
    "    pltstyle.use(style if style else 'seaborn-deep')\n",
    "    fig = plt.figure(figsize = (16,8))\n",
    "    ax = fig.add_axes((0.1, 0.2, 0.8, 0.7))\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "    plt.plot(tra, label=f\"Train Acc\", linewidth=3)\n",
    "    plt.plot(vla, label=f\"Valid Acc\", linewidth=3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    if not save_dir is None:\n",
    "        # Dumping the plot as well\n",
    "        pltstyle.use(style if style else 'seaborn-deep')\n",
    "        fig = plt.figure(figsize = (16,8))\n",
    "        ax = fig.add_axes((0.1, 0.2, 0.8, 0.7))\n",
    "        ax.spines['right'].set_color('none')\n",
    "        ax.spines['top'].set_color('none')\n",
    "    #     plt.xticks([])\n",
    "    #     plt.yticks([])\n",
    "        plt.plot(tra, label=f\"Train Acc\", linewidth=3)\n",
    "        plt.plot(vla, label=f\"Valid Acc\", linewidth=3)\n",
    "        plt.legend()\n",
    "        plt.savefig(save_dir/'sup_acc.png')\n",
    "    \n",
    "def plot(trcs):\n",
    "    layers = len(trcs[0])\n",
    "    for l in range(layers):\n",
    "        plt.plot(trcs[:,l], label=f\"layer {l}\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_accs(traces[0], traces[1], save_dir=UNSUP_MODEL_DIR if not SAFE_MODE else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_best = np.max(traces[0])\n",
    "trn_best_ = np.argmax(traces[0])\n",
    "val_attrn = traces[1][trn_best_]\n",
    "val_best = np.max(traces[1])\n",
    "val_best_ = np.argmax(traces[1])\n",
    "print(f\"Train Best: {trn_best:.4f} at {trn_best_}\\nValid @Trn: {val_attrn:.4f}\\nValid Best: {val_best:.4f} at {val_best_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
