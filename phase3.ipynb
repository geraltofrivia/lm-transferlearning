{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# pretty print all cell's output and not just the last one\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Currently Running:\n",
    "DANN with lambda 5.0, dir 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Lib imports\n",
    "import os\n",
    "import re\n",
    "import html\n",
    "import pickle\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "os.environ['QT_QPA_PLATFORM'] = 'offscreen'\n",
    "\n",
    "# FastAI Imports\n",
    "from fastai import text, core, lm_rnn\n",
    "\n",
    "# Torch imports\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Mytorch imports\n",
    "from mytorch import loops, lriters as mtlr, dataiters as mtdi\n",
    "from mytorch.utils.goodies import *\n",
    "\n",
    "# Local imports\n",
    "from options import Phase3 as params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUICK, DEBUG, MODEL_NUM = False, True, '12'\n",
    "PRETRAINED = True\n",
    "UNSUP_MODEL_DIR = PATH / 'models' / MODEL_NUM\n",
    "MODEL_SUFFIX = '_lowaux'\n",
    "\n",
    "assert MODEL_SUFFIX in ['_lowaux', '_hightrn', ''], 'Incorrect Suffix given with which to load model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     58,
     66
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f292343b330>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\n    Paths and macros\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\n    Model code\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\n    Prepare data\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "'''\n",
    "    Paths and macros\n",
    "'''\n",
    "# Path fields\n",
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "\n",
    "DATA_PATH = Path('raw/imdb/aclImdb/')\n",
    "DATA_PATH.mkdir(exist_ok=True)\n",
    "PATH = Path('resources/proc/imdb')\n",
    "DATA_PROC_PATH = PATH / 'data'\n",
    "DATA_LM_PATH = PATH / 'datalm'\n",
    "\n",
    "LM_PATH = Path('resources/models')\n",
    "LM_PATH.mkdir(exist_ok=True)\n",
    "PRE_PATH = LM_PATH / 'wt103'\n",
    "PRE_LM_PATH = PRE_PATH / 'fwd_wt103.h5'\n",
    "CLASSES = ['neg', 'pos', 'unsup']\n",
    "\n",
    "\n",
    "'''\n",
    "    Model code\n",
    "'''\n",
    "class CustomEncoder(lm_rnn.MultiBatchRNN):\n",
    "    @property\n",
    "    def layers(self):\n",
    "        # TODO: ADD ENCODERR!!!!!!!!!!\n",
    "        return torch.nn.ModuleList([torch.nn.ModuleList([self.rnns[0], self.dropouths[0]]),\n",
    "                                    torch.nn.ModuleList([self.rnns[1], self.dropouths[1]]),\n",
    "                                    torch.nn.ModuleList([self.rnns[2], self.dropouths[2]])])\n",
    "\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "\n",
    "    # @TODO: inject comments.\n",
    "    def __init__(self,\n",
    "                 _device: torch.device,\n",
    "                 ntoken: int,\n",
    "                 dps: list,\n",
    "                 enc_wgts = None,\n",
    "                 _debug=False):\n",
    "        super(TextClassifier, self).__init__()\n",
    "\n",
    "        self.device = _device\n",
    "\n",
    "        # Load the pre-trained model\n",
    "        args = {'ntoken': ntoken, 'emb_sz': 400, 'n_hid': 1150,\n",
    "                'n_layers': 3, 'pad_token': 0, 'qrnn': False, 'bptt': 70, 'max_seq': 1400,\n",
    "                'dropouti': dps[0], 'wdrop': dps[1], 'dropoute': dps[2], 'dropouth': dps[3]}\n",
    "        self.encoder = CustomEncoder(**args).to(self.device)\n",
    "        if enc_wgts:\n",
    "            self.encoder.load_state_dict(enc_wgts)\n",
    "        '''\n",
    "            Make new classifier.\n",
    "            \n",
    "            Explanation:\n",
    "                400*3 because input is [ h_T, maxpool, meanpool ]\n",
    "                50 is hidden layer dim\n",
    "                2 is n_classes\n",
    "\n",
    "                0.4, 0.1 are drops at various layers\n",
    "        '''\n",
    "        self.linear = text.PoolingLinearClassifier(layers=[400 * 3, 50, 2], drops=[dps[4], 0.1]).to(self.device)\n",
    "        self.encoder.reset()\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        layers = [x for x in self.encoder.layers]\n",
    "        layers += [x for x in self.linear.layers]\n",
    "        return torch.nn.ModuleList(layers)\n",
    "\n",
    "    @property\n",
    "    def layers_rev(self):\n",
    "        layers = [x for x in self.encoder.layers]\n",
    "        layers += [x for x in self.linear.layers]\n",
    "        layers.reverse()\n",
    "        return torch.nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # inputs are S*B\n",
    "\n",
    "        # Encoding all the data\n",
    "        op_p = self.encoder(x.transpose(1, 0))\n",
    "        # pos_batch = op_p[1][-1][-1]\n",
    "        score = self.linear(op_p)[0]\n",
    "\n",
    "        return score\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            predicted = self.forward(x)\n",
    "            self.train()\n",
    "            return predicted\n",
    "\n",
    "\n",
    "'''\n",
    "    Prepare data\n",
    "'''\n",
    "re1 = re.compile(r'  +')\n",
    "\n",
    "\n",
    "def fixup(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
    "        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "\n",
    "def get_texts(df, n_lbls=1):\n",
    "    labels = df.iloc[:, range(n_lbls)].values.astype(np.int64)\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df.iloc[:, 1].astype(str)\n",
    "    texts = list(texts.apply(fixup).values)\n",
    "\n",
    "    tok = text.Tokenizer().proc_all_mp(core.partition_by_cores(texts))\n",
    "    return tok, list(labels)\n",
    "\n",
    "\n",
    "def get_all(df, n_lbls):\n",
    "    tok, labels = get_texts(df)\n",
    "    return tok, labels\n",
    "\n",
    "\n",
    "def get_texts_org(path):\n",
    "    texts, labels = [], []\n",
    "    for idx, label in enumerate(CLASSES):\n",
    "        for fname in (path / label).glob('*.*'):\n",
    "            texts.append(fname.open('r', encoding='utf-8').read())\n",
    "            labels.append(idx)\n",
    "    return np.array(texts), np.array(labels)\n",
    "\n",
    "\n",
    "def epoch_end_hook() -> None:\n",
    "    lr_schedule.reset()\n",
    "\n",
    "\n",
    "def eval(y_pred, y_true):\n",
    "    \"\"\"\n",
    "        Expects a batch of input\n",
    "\n",
    "        :param y_pred: tensor of shape (b, nc)\n",
    "        :param y_true: tensor of shape (b, 1)\n",
    "    \"\"\"\n",
    "    return torch.mean((torch.argmax(y_pred, dim=1) == y_true).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Make model\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\n    Setup things for training (data, loss, opt, lr schedule etc\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_texts, trn_labels = get_texts_org(DATA_PATH / 'train')\n",
    "val_texts, val_labels = get_texts_org(DATA_PATH / 'test')\n",
    "\n",
    "# Lose label 2 from train\n",
    "trn_texts = trn_texts[trn_labels<2]\n",
    "trn_labels = trn_labels[trn_labels<2]\n",
    "\n",
    "# Shuffle data\n",
    "if QUICK:\n",
    "    np.random.seed(42)\n",
    "    trn_idx = np.random.permutation(len(trn_texts))[:1000]\n",
    "    val_idx = np.random.permutation(len(val_texts))[:1000]\n",
    "else:\n",
    "    np.random.seed(42)\n",
    "    trn_idx = np.random.permutation(len(trn_texts))\n",
    "    val_idx = np.random.permutation(len(val_texts))\n",
    "\n",
    "trn_texts, trn_labels = trn_texts[trn_idx], trn_labels[trn_idx]\n",
    "val_texts, val_labels = val_texts[val_idx], val_labels[val_idx]\n",
    "col_names = ['labels', 'text']\n",
    "\n",
    "df_trn = pd.DataFrame({'text': trn_texts, 'labels': trn_labels}, columns=col_names)\n",
    "df_val = pd.DataFrame({'text': val_texts, 'labels': val_labels}, columns=col_names)\n",
    "\n",
    "itos_path = UNSUP_MODEL_DIR / 'itos.pkl'\n",
    "itos2 = pickle.load(itos_path.open('rb'))\n",
    "stoi2 = {v: k for k, v in enumerate(itos2)}\n",
    "\n",
    "trn_clas, trn_labels = get_all(df_trn, 1)\n",
    "val_clas, val_labels = get_all(df_val, 1)\n",
    "\n",
    "trn_clas = np.array([[stoi2.get(w, 0) for w in para] for para in trn_clas])\n",
    "val_clas = np.array([[stoi2.get(w, 0) for w in para] for para in val_clas])\n",
    "trn_labels = [x for y in trn_labels for x in y]\n",
    "val_labels = [x for y in val_labels for x in y]\n",
    "\n",
    "'''\n",
    "    Make model\n",
    "'''\n",
    "dps = list(params.encoder_dropouts)\n",
    "# enc_wgts = torch.load(LM_PATH, map_location=lambda storage, loc: storage)\n",
    "enc_wgts = torch.load(UNSUP_MODEL_DIR / 'unsup_model_enc'+MODEL_SUFFIX+'.torch', map_location=lambda storage, loc: storage)\n",
    "clf = TextClassifier(device, len(itos2), dps, enc_wgts=enc_wgts if PRETRAINED else None)\n",
    "\n",
    "'''\n",
    "    Setup things for training (data, loss, opt, lr schedule etc\n",
    "'''\n",
    "bs = params.bs\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "opt_fn = partial(optim.Adam, betas=params.adam_betas)\n",
    "opt = make_opt(clf, opt_fn, lr=0.001)\n",
    "opt.param_groups[-1]['lr'] = 0.01\n",
    "\n",
    "# Make data\n",
    "data_fn = partial(mtdi.SortishSampler, _batchsize=bs, _padidx=1)\n",
    "data = {'train': {'x': trn_clas, 'y': trn_labels}, 'valid': {'x': val_clas, 'y': val_labels}}\n",
    "\n",
    "# Make lr scheduler\n",
    "lr_args = {'iterations': len(data_fn(data['train'])), 'cycles': 1}\n",
    "lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.CosineAnnealingLR)\n",
    "\n",
    "args = {'epochs': 1, 'data': data, 'device': device,\n",
    "        'opt': opt, 'loss_fn': loss_fn, 'model': clf,\n",
    "        'train_fn': clf, 'predict_fn': clf.predict,\n",
    "        'epoch_end_hook': epoch_end_hook, 'weight_decay': params.weight_decay,\n",
    "        'clip_grads_at': params.clip_grads_at, 'lr_schedule': lr_schedule,\n",
    "        'data_fn': data_fn, 'eval_fn': eval}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Training schedule:\\n    \\n    1. Unfreeze one layer. Train for 1 epoch\\n    2 - 5. Unfreeze one layer, train for 1 epoch\\n    3. Train for 15 epochs (after all layers are unfrozen). Use 15 cycles for cosine annealing.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1042/1042 [05:01<00:00,  3.43it/s]\n",
      "100%|██████████| 1042/1042 [01:52<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000 | Loss: 0.31983 | Tr_c: 0.86428 | Vl_c: 0.91503 | Time: 5.050 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1042/1042 [05:01<00:00,  3.78it/s]\n",
      "100%|██████████| 1042/1042 [01:50<00:00,  9.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000 | Loss: 0.23688 | Tr_c: 0.90811 | Vl_c: 0.91479 | Time: 5.049 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1042/1042 [05:02<00:00,  3.19it/s]\n",
      "100%|██████████| 1042/1042 [01:51<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000 | Loss: 0.23675 | Tr_c: 0.91001 | Vl_c: 0.91769 | Time: 5.066 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 600/1042 [02:57<02:38,  2.79it/s]ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/data/priyansh/conda/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-c285844546d7>\", line 32, in <module>\n",
      "    traces_new = loops.generic_loop(**args)\n",
      "  File \"/data/priyansh/lm-transferlearning/mytorch/loops.py\", line 187, in generic_loop\n",
      "    _x = torch.tensor(x, dtype=torch.long, device=device)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/priyansh/conda/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/priyansh/conda/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/data/priyansh/conda/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/data/priyansh/conda/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/data/priyansh/conda/anaconda3/envs/fastai/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/data/priyansh/conda/anaconda3/envs/fastai/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/data/priyansh/conda/anaconda3/envs/fastai/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/data/priyansh/conda/anaconda3/envs/fastai/lib/python3.6/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/data/priyansh/conda/anaconda3/envs/fastai/lib/python3.6/inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/data/priyansh/conda/anaconda3/envs/fastai/lib/python3.6/posixpath.py\", line 385, in abspath\n",
      "    return normpath(path)\n",
      "  File \"/data/priyansh/conda/anaconda3/envs/fastai/lib/python3.6/posixpath.py\", line 359, in normpath\n",
      "    comps = path.split(sep)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Training schedule:\n",
    "    \n",
    "    1. Unfreeze one layer. Train for 1 epoch\n",
    "    2 - 5. Unfreeze one layer, train for 1 epoch\n",
    "    3. Train for 15 epochs (after all layers are unfrozen). Use 15 cycles for cosine annealing.\n",
    "'''\n",
    "# opt.param_groups[-1]['lr'] = 0.01\n",
    "traces = loops.generic_loop(**args)\n",
    "\n",
    "opt.param_groups[-1]['lr'] = 0.01\n",
    "opt.param_groups[-2]['lr'] = 0.005\n",
    "lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.CosineAnnealingLR)\n",
    "args['lr_schedule'] = lr_schedule\n",
    "traces_new = loops.generic_loop(**args)\n",
    "traces = [a+b for a, b in zip(traces, traces_new)]\n",
    "\n",
    "opt.param_groups[-1]['lr'] = 0.01\n",
    "opt.param_groups[-2]['lr'] = 0.005\n",
    "opt.param_groups[-3]['lr'] = 0.001\n",
    "lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.CosineAnnealingLR)\n",
    "args['lr_schedule'] = lr_schedule\n",
    "traces_new = loops.generic_loop(**args)\n",
    "traces = [a+b for a, b in zip(traces, traces_new)]\n",
    "\n",
    "opt.param_groups[-1]['lr'] = 0.01\n",
    "opt.param_groups[-2]['lr'] = 0.005\n",
    "opt.param_groups[-3]['lr'] = 0.001\n",
    "opt.param_groups[-4]['lr'] = 0.001\n",
    "lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.CosineAnnealingLR)\n",
    "args['lr_schedule'] = lr_schedule\n",
    "traces_new = loops.generic_loop(**args)\n",
    "traces = [a+b for a, b in zip(traces, traces_new)]\n",
    "\n",
    "opt.param_groups[-1]['lr'] = 0.01\n",
    "opt.param_groups[-2]['lr'] = 0.005\n",
    "opt.param_groups[-3]['lr'] = 0.001\n",
    "opt.param_groups[-4]['lr'] = 0.001\n",
    "opt.param_groups[-5]['lr'] = 0.001\n",
    "lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.CosineAnnealingLR)\n",
    "args['lr_schedule'] = lr_schedule\n",
    "traces_new = loops.generic_loop(**args)\n",
    "traces = [a+b for a, b in zip(traces, traces_new)]\n",
    "\n",
    "opt.param_groups[-1]['lr'] = 0.01\n",
    "opt.param_groups[-2]['lr'] = 0.005\n",
    "opt.param_groups[-3]['lr'] = 0.001\n",
    "opt.param_groups[-4]['lr'] = 0.001\n",
    "opt.param_groups[-5]['lr'] = 0.001\n",
    "lr_args['cycles'] = 15\n",
    "args['epochs'] = 15\n",
    "lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.CosineAnnealingLR)\n",
    "args['lr_schedule'] = lr_schedule\n",
    "traces_new = loops.generic_loop(**args)\n",
    "traces = [a+b for a, b in zip(traces, traces_new)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_save(UNSUP_MODEL_DIR,\n",
    "            torch_stuff=[tosave('sup_model.torch', clf.state_dict())],\n",
    "            pickle_stuff=[tosave('final_sup_traces.pkl', traces), tosave('unsup_options.pkl', params)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNSUP_MODEL_DIR"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def plot(trcs):\n",
    "    layers = len(trcs[0])\n",
    "    for l in range(layers):\n",
    "        plt.plot(trcs[:,l], label=f\"layer {l}\")\n",
    "    plt.show()\n",
    "\n",
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "opt = make_opt(clf, opt_fn, lr=0.01)\n",
    "opt.param_groups[-1]['lr'] = 0.01\n",
    "# lr_args = {'iterations': len(data_fn(data['train'])), 'cycles': 1}\n",
    "lr_args = {'length': len(data_fn(data['train']))}\n",
    "lr_schedule = mtlr.LearningRateScheduler(opt, lr_args, mtlr.ConstantLR)\n",
    "lrss = [lr_schedule.get() for _ in range(lr_args['length'])]\n",
    "plot(np.asarray(lrss))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lr_args = {'iterations': len(data_fn(data['train'])), 'cycles': 1}\n",
    "lr_schedule = mtlr.LearningRateScheduler(opt, lr_args, mtlr.CosineAnnealingLR)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot(trcs):\n",
    "    layers = len(trcs[0])\n",
    "    for l in range(layers):\n",
    "        plt.plot(trcs[:,l], label=f\"layer {l}\")\n",
    "    plt.show()\n",
    "    \n",
    "plot(np.asarray(traces[-1][100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(np.asarray([[x[-1]] for x in traces[-1][:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr_args)\n",
    "lr_args['cycles'] = 5\n",
    "lr_args['iterations'] = 42*15\n",
    "lr_schedule = mtlr.LearningRateScheduler(optimizer=opt, lr_args=lr_args, lr_iterator=mtlr.CosineAnnealingLR)\n",
    "lrs = []\n",
    "while True:\n",
    "    try:\n",
    "        lrs.append(lr_schedule.get())\n",
    "    except CustomError:\n",
    "        break\n",
    "plot(np.asarray(lrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style as pltstyle\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (16, 8)\n",
    "\n",
    "def plot_accs(tra, vla, style=None):\n",
    "    pltstyle.use(style if style else 'seaborn-deep')\n",
    "    fig = plt.figure(figsize = (16,8))\n",
    "    ax = fig.add_axes((0.1, 0.2, 0.8, 0.7))\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "    plt.plot(tra, label=f\"Train Acc\", linewidth=3)\n",
    "    plt.plot(vla, label=f\"Valid Acc\", linewidth=3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot(trcs):\n",
    "    layers = len(trcs[0])\n",
    "    for l in range(layers):\n",
    "        plt.plot(trcs[:,l], label=f\"layer {l}\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_accs(traces[0], traces[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
